\subsubsection{Convergence of Jacobi and Gauss-Seidel methods}

Let be a general matrix $A$, and :
\begin{itemize}
    \item $D$ the \textbf{diagonal part} of $A$
    \item $-E$ \textbf{lower triangular part} of $A$
    \item $-F$ \textbf{upper triangular part} of $A$
\end{itemize}
\begin{equation*}
    A = \begin{bmatrix}
        & & & & \\
        & \ddots &   & -F     & \\
        &        & D &        & \\
        & -E     &   & \ddots & \\
        & & & &
    \end{bmatrix}
\end{equation*}
The previous Jacobi and Gauss-Seidel methods can be rewritten as:
\begin{itemize}
    \item Jacobi:
    \begin{itemize}
        \item Method:
        \begin{equation*}
            D\mathbf{x}^{\left(k+1\right)} = \left(E+F\right)\mathbf{x}^{\left(k\right)} + \mathbf{b}
        \end{equation*}
        \item Iteration matrix:
        \begin{equation*}
            B_{J} = D^{-1}\left(E+F\right) = D^{-1}\left(D-A\right) = I-D^{-1}A
        \end{equation*}
    \end{itemize}

    \item Gauss-Seidel
    \begin{itemize}
        \item Method:
        \begin{equation*}
            \left(D-E\right)\mathbf{x}^{\left(k+1\right)} = F\mathbf{x}^{k} + \mathbf{b}
        \end{equation*}
        \item Iteration matrix:
        \begin{equation*}
            B_{GS} = \left(D-E\right)^{-1}F
        \end{equation*}
    \end{itemize}
\end{itemize}
We present a theorem which gives us the \textbf{sufficient condition for convergence} of the Jacobi and Gauss-Seidel methods.

\begin{theorem}[\textbf{sufficient condition for convergence of Jacobi and Gauss-Seidel}]
    The following conditions are sufficient for convergence:
    \begin{itemize}
        \item If a matrix $A$ is \textbf{strictly diagonally dominant by \underline{rows}}:
        \begin{equation*}
            \left|a_{ii}\right| > \displaystyle\sum_{j \ne i} \left|a_{ij}\right| \hspace{2em} i = 1, \dots, n
        \end{equation*}
        Then Jacobi and Gauss-Seidel converge.

        \item If a matrix $A$ is \textbf{strictly diagonally dominant by \underline{columns}}:
        \begin{equation*}
            \left|a_{ii}\right| > \displaystyle\sum_{j \ne i} \left|a_{ji}\right| \hspace{2em} i = 1, \dots, n
        \end{equation*}
        Then Jacobi and Gauss-Seidel converge.

        \item If a matrix $A$ is SPD (symmetric positive and definite), then the Gauss-Seidel method is convergent.
        
        \item If a matrix $A$ is tridiagonal\footnote{A matrix is \definitionWithSpecificIndex{tridiagonal}{Tridiagonal matrix} when it has non-zero elements only on the main diagonal, the diagonal above the main diagonal, and the diagonal below the main diagonal.
        \begin{equation*}
            A = \begin{bmatrix}
                a_{1,1} & a_{1,2} & 0       & 0  \\
                a_{2,1} & a_{2,2} & a_{2,3} & 0  \\
                0 & a_{3,2} & a_{3,3} & a_{3,4} \\
                0 & 0 & a_{4,3} & a_{4,4} \\
            \end{bmatrix}
        \end{equation*}
        }, then the square spectral value of the Jacobi iteration matrix is equal to the spectral value of the Gauss-Seidel iteration matrix.
        \begin{equation*}
            \rho^{2}\left(B_{J}\right) = \rho\left(B_{GS}\right)
        \end{equation*}
    \end{itemize}
\end{theorem}
