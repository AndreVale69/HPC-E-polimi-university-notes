\section{Iterative methods for linear systems of equations}

\subsection{Why not use the direct methods?}

Let us considering the following linear system of equations:
\begin{equation*}
    Ax = b
\end{equation*}
Where $A \in \mathbb{R}^{n \times n}$, $b \in \mathbb{R}^{n}$, $x \in \mathbb{R}^{n}$ and $\det\left(A\right) \ne 0$. In general, direct methods are \textbf{not very suitable whenever}:
\begin{itemize}
    \item \textbf{$n$ is large}. Typically, the average cost of direct methods scales as $n^{3}$, except in selected cases. As a trivial example, if peak performance is 1 PetaFLOPS ($10^{15}$ floating point operations per second), then
    \begin{equation*}
        n = 10^{7} \rightarrow \approx 10^{6} \text{ seconds} \approx 11 \text{ days}
    \end{equation*}
    \item \textbf{Matrix $A$ is sparse}. Direct methods suffer from the \emph{fill-in} phenomenon\footnote{The fill-in of a matrix are those entries that change from an initial zero to a non-zero value during the execution of an algorithm. To reduce the memory requirements and the number of arithmetic operations used during an algorithm, it is useful to minimize the fill-in.} (see later). Unfortunately, sparse matrices are very popular in many application problems and we cannot consider them.
\end{itemize}

\highspace
\begin{definitionbox}[: Sparse Matrix]
    Let $A \in \mathbb{R}^{n \times n}$ we say that $A$ is \definitionWithSpecificIndex{sparse}{Sparse Matrix} the number of non-zero elements (abbreviated as $\nnz\left(A\right)$) is approximately equal to the number of rows/columns $n$, i.e. $\nnz\left(A\right) \sim n$.
\end{definitionbox}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{What is an iterative method?}}
\end{flushleft}
It is clear that iterative methods are usually better than direct methods. An \definitionWithSpecificIndex{iterative method}{Iterative Method} is a \textbf{mathematical procedure that uses an initial value to generate a sequence of improving approximate solutions to a class of problems}, where the $i$-th approximation (called an \dquotes{\emph{iteration}}) is derived from the previous ones.

\highspace
More precisely, we introduce a sequence $\mathbf{x}^{\left(k\right)}$ of vectors determined by a recursive relation that identifies the method.
\begin{equation*}
    \mathbf{x}^{\left(0\right)} \rightarrow
    \mathbf{x}^{\left(1\right)} \rightarrow
    \cdots \rightarrow
    \mathbf{x}^{\left(k\right)} \rightarrow
    \mathbf{x}^{\left(k+1\right)} \rightarrow
    \cdots
\end{equation*}
To \dquotes{\emph{initialize}} the iterative process, it is necessary to provide a starting point (\emph{initial vector}, also called \emph{initial guess}) $\mathbf{x}^{\left(0\right)}$, e.g. based on physical/engineering applications.

\newpage

\noindent
After initialization, the core of the process should, sooner or later, produce a result. It is a very complex and long topic, but in general it refers to the process by which an iterative algorithm approaches a fixed point or a solution to a problem after several iterations. An \textbf{iterative method must satisfy the} \definitionWithSpecificIndex{convergence property}{Convergence property}:
\begin{equation}\label{eq: convergence property}
    \lim\limits_{k \rightarrow + \infty} \mathbf{x}^{\left(k\right)} = \mathbf{x}
\end{equation}
It is important to note that the \textbf{convergence \underline{does not depend} on the choice of the initial vector} $x^{\left(0\right)}$.

\highspace
From the property \ref{eq: convergence property}, it should be clear that \textbf{convergence is guaranteed only after an $\infty$ number of iterations}. From a practical point of view, we need to stop the iteration process after a finite number of iterations when we are \emph{sufficiently close} to the solution.

\highspace
In addition to the \emph{problem of convergence} and \dquotes{\emph{when should we stop our convergence method}}, we have to deal with the \emph{numerical error} inevitably introduced by our method.

\highspace
These topics will be explained and faced in the following pages.