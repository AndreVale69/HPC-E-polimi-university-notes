\subsection{Linear iterative methods}

\subsubsection{Definition}

In general, we consider linear iterative methods of the following form:
\begin{equation*}
    \mathbf{x}^{\left(k+1\right)} = B\mathbf{x}^{\left(k\right)} + \mathbf{f} \hspace{2em} k \ge 0
\end{equation*}
Where $B \in \mathbb{R}^{n \times n}$, $\mathbf{f} \in \mathbb{R}^{n}$ and the matrix $B$ is called \textbf{iteration matrix}. The choice of the iteration matrix and $\mathbf{f}$ uniquely identifies the method.

\highspace
The question is now automatic. \textbf{How to choose} an intelligent iteration matrix and $\mathbf{f}$? There are two main factors to consider:
\begin{itemize}
    \item \textbf{\underline{Consistency}}. This is a necessary condition, but not sufficient to guarantee the convergence. If $\mathbf{x}^{\left(k\right)}$ es the exact solution $\mathbf{x}$, then $\mathbf{x}^{\left(k+1\right)}$ is again equal to $\mathbf{x}$ (no update if the exact solution is found):
    \begin{equation*}
        \mathbf{x} = B \mathbf{x} + \mathbf{f} \longrightarrow \mathbf{f} = \left(I-B\right)\mathbf{x} = \left(I-B\right)A^{-1}\mathbf{b}
    \end{equation*}
    The former identity gives a relationship between $B$ and $\mathbf{f}$ as a function of the data.

    \item \textbf{\underline{Convergence}}. To study the convergence we need the error and the spectral radius:
    \begin{itemize}
        \item \textbf{Error}. Let us introduce the error at step $\left(k+1\right)$:
        \begin{equation*}
            \mathbf{e}^{\left(k+1\right)} = \mathbf{x} - \mathbf{x}^{\left(k+1\right)}
        \end{equation*}
        And an appropriate vector norm, such as the Euclidean norm $\vectorNormSymbol$.
        
        Then we have:
        \begin{equation*}
            \begin{array}{rcl}
                \left|\left|\mathbf{e}^{\left(k+1\right)}\right|\right| &=& \left|\left|\mathbf{x}-\mathbf{x}^{\left(k+1\right)}\right|\right| \\ [.5em]
                %
                &=& \left|\left|\mathbf{x}-\left(B\mathbf{x}^{\left(k\right)} + \mathbf{f}\right)\right|\right| \\ [.5em]
                %
                &=& \left|\left|\mathbf{x}-B\mathbf{x}^{\left(k\right)} - \mathbf{f}\right|\right| \\ [.5em]
                %
                &=& \left|\left|\mathbf{x}-B\mathbf{x}^{\left(k\right)} - \left(I-B\right)\mathbf{x}\right|\right| \\ [.5em]
                %
                &=& \left|\left|\mathbf{x}-B\mathbf{x}^{\left(k\right)} - I\mathbf{x} + B\mathbf{x}\right|\right| \\ [.5em]
                %
                &=& \left|\left|\mathbf{x}-B\mathbf{x}^{\left(k\right)} - \mathbf{x} + B\mathbf{x}\right|\right| \\ [.5em]
                %
                &=& \left|\left|-B\mathbf{x}^{\left(k\right)} + B\mathbf{x}\right|\right| \\ [.5em]
                %
                &=& \left|\left|B\left(\mathbf{x} - \mathbf{x}^{\left(k\right)}\right)\right|\right| \\ [.5em]
                %
                &=& \left|\left|B\mathbf{e}^{\left(k\right)}\right|\right| \\ [.5em]
                %
                &\le& \left|\left|B\right|\right| \cdot \left|\left|\mathbf{e}^{\left(k\right)}\right|\right|
            \end{array}
        \end{equation*}
        Note that $\left|\left|B\right|\right|$ is the matrix norm induced by the vector norm $\vectorNormSymbol$.

        Using recursion, we get:
        \begin{equation*}
            \begin{array}{rcl}
                \left|\left|\mathbf{e}^{\left(k+1\right)}\right|\right| &\le& \left|\left|B\right|\right| \cdot \left|\left|\mathbf{e}^{\left(k\right)}\right|\right| \\ [.5em]
                %
                &\le& \left|\left|B\right|\right| \cdot \left|\left|B\right|\right| \cdot \left|\left|\mathbf{e}^{\left(k-1\right)}\right|\right| \\ [.5em]
                %
                &\le& \left|\left|B\right|\right| \cdot \left|\left|B\right|\right| \cdot \left|\left|B\right|\right| \cdot \left|\left|\mathbf{e}^{\left(k-2\right)}\right|\right| \\ [.5em]
                %
                &\le& \cdots \\ [.5em]
                %
                &\le& {\left|\left|B\right|\right|}^{\left(k+1\right)} \cdot \left|\left|\mathbf{e}^{\left(0\right)}\right|\right| \\ [.5em]
                %
                \lim\limits_{k \rightarrow \infty} \left|\left|\mathbf{e}^{\left(k+1\right)}\right|\right| &\le& \left(\lim\limits_{k \rightarrow \infty} {\left|\left|B\right|\right|}^{\left(k+1\right)}\right) \cdot \left|\left|\mathbf{e}^{\left(0\right)}\right|\right|
            \end{array}
        \end{equation*}
        And here is the key. The \textbf{sufficient condition for convergence is to choose a matrix $B$ that has the norm less than $1$}:
        \begin{equation*}
            \left|\left|B\right|\right| < 1 \Longrightarrow \lim\limits_{k \rightarrow \infty} \left|\left|\mathbf{e}^{\left(k+1\right)}\right|\right| = 0
        \end{equation*}
        We recall that the \emph{Euclidean norm} (commonly used) of a matrix is calculated by taking the square root of the sum of the absolute squares of its elements. Let $A$ be a matrix of size $m \times n$, the Euclidean norm:
        \begin{equation*}
            {\left|\left|A\right|\right|}_{2} \equiv \sqrt{\displaystyle\sum_{i=1}^{m}\sum_{j=1}^{n} {\left|a_{ij}\right|}^{2}}
        \end{equation*}
        
        
        \item \textbf{Spectral radius}. The spectral radius of a matrix is the \textbf{largest absolute value of its eigenvalues}. We define:
        \begin{equation*}
            \rho\left(B\right) = \underset{j}{\max} \left|\lambda_{j}\left(B\right)\right|
        \end{equation*}
        Where $\lambda_{j}\left(B\right)$ are the eigenvalues of $B$.

        Why is the spectral radius useful? Well, if the matrix $B$ is symmetric positive definite (SPD)\footnote{\definition{SPD (Symmetric Positive Definite)} is a matrix: \begin{itemize}
            \item Symmetric: $A = A^{T}$
            \item Positive Definite: $x^{T}AX > 0$, $\forall x \in \mathbb{R}^{n} \setminus \left\{0\right\}$
        \end{itemize}}, then the spectral radius is equal to the Euclidean norm of the matrix.
        \begin{equation*}
            B \text{ is SPD } \Rightarrow {\left|\left|B\right|\right|}_{2} = \rho\left(B\right) \: \land \: \rho\left(B\right) < 1 \iff \text{method convergences}
        \end{equation*}
        And this is a very big help to us for many reasons.
        \begin{itemize}
            \item \textbf{Balance and Predictability}. When the norm is equal to the spectral, it means that the influence of the matrix is well distributed. In other words, this uniformity can help make our iterative methods more predictable, reducing the possibility of non-convergence.

            \item \textbf{Efficiency}. It avoids scenarios where the matrix might have hidden large entries affecting convergence or stability.
        \end{itemize}   
    \end{itemize}
\end{itemize}
\newpage
\noindent
Let $C \in \mathbb{R}^{n \times n}$ then the spectral radius of a matrix is equal to the \href{https://en.wikipedia.org/wiki/Infimum_and_supremum}{infimum} (lower bound) of its matrix norm:
\begin{equation}
    \rho\left(C\right) = \mathrm{inf}\left\{ \left|\left|C\right|\right| \:\: \forall \text{ induced matrix norm }\vectorNormSymbol\right\}
\end{equation}
It follows from this property that:
\begin{equation}\label{eq: spectral radius less/equal than norm}
    \rho\left(B\right) \le \left|\left|B\right|\right| \hspace{2em} \forall \text{induced matrix norm } \vectorNormSymbol
\end{equation}
Note that thanks to \ref{eq: spectral radius less/equal than norm} we can observe that if:
\begin{equation*}
    \exists \vectorNormSymbol \: \text{such that} \: \left|\left|B\right|\right| < 1 \Longrightarrow \rho\left(B\right) < 1
\end{equation*}
The convergence of the method is guaranteed by the following theorem.

\highspace
\begin{theorem}[\textbf{necessary and sufficient condition for convergence}]
    A \textbf{consistent} iterative method with iteration matrix B converges if and only if $\rho\left(B\right) < 1$.
\end{theorem}

\newpage

\subsubsection{Jacobi method}

Let the problem of solve $Ax = b$, where $A$ is a square matrix, $x$ is the vector of unknowns, and $b$ is the result vector.

\highspace
We start from the $i$-th line of the linear system:
\begin{equation*}
    \displaystyle\sum_{j = 1}^{n} a_{ij}x_{j} = b_{i} \: \rightarrow \: a_{i1}x_{1} + a_{i2}x_{2} + \cdots + a_{in}x_{n} = b_{i}
\end{equation*}
Formally the solution $x_{i}$ for each $i$ si given by:
\begin{equation}
    x_{i} = \dfrac{b_{i}-\displaystyle\sum_{j \ne i} a_{ij}x_{j}}{a_{ii}}
\end{equation}
Obviously the previous identity cannot be used in practice because we do not know $x_{j}$, for $j \ne i$. And here is the \textbf{magic idea} of Jacobi: we could think of introducing an iterative method (Jacobi) that \textbf{updates} $x_{i}^{\left(k+1\right)}$ \textbf{step} $k+1$ \textbf{using the other} $x_{j}^{\left(k\right)}$ \textbf{obtained in the previous step} $k$.
\begin{equation}\label{eq: jacobi x calcolus}
    x_{i} = \dfrac{b_{i}-\displaystyle\sum_{j \ne i} a_{ij}x_{j}}{a_{ii}} \: \xRightarrow{\text{as }x_{j}\text{ is not well known}} \: x_{i}^{\left(k+1\right)} = \dfrac{b_{i}-\displaystyle\sum_{j \ne i} a_{ij}x_{j}^{\left(k\right)}}{a_{ii}}
\end{equation}
Where $\forall i = 1, \dots, n$.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{tools} \textbf{Algorithm}}
\end{flushleft}
\begin{enumerate}
    \item \textbf{Start with an initial guess} $x^{\left(0\right)}$, also zero.
    \item \textbf{Update each component} $x_{i}^{\left(k+1\right)}$ using the equation \ref{eq: jacobi x calcolus}.
    \item \textbf{Repeat until the changes are less than a specified tolerance} or we haven't found the exact solution (in practice very difficult, almost impossible).
\end{enumerate}

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{dollar-sign} \textbf{How much does it cost?}}
    \label{general-ref: cost jacobi method}
\end{flushleft}
It depends on the matrix used:
\begin{itemize}
    \item \textbf{Dense matrix} (bad choice). Each iteration costs $\approx n^{2}$ operations, so the Jacobi method is competitive if the number of iteration is less than $n$.
    \item \textbf{Sparse matrix} (good choice). Each iteration costs only $\approx n$ operations.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{network-wired} \textbf{Can it be parallelized?}}
\end{flushleft}
The parallelization of the Jacobi method is actually \textbf{one of its main advantages} on modern computers. Each update of $x_{i}$ depends only on the previous values of the other $x_{j}$, not on the current iteration values. This independence makes it easy to distribute the work across multiple processors.

\newpage

\subsubsection{Gauss-Seidel method}

Given the Jacobi method, the Gauss Seidel method is similar, but with one clever difference: it uses the latest available values during iterations.
\begin{equation}\label{eq: gauss seidel x calcolus}
    x_{i}^{\left(k+1\right)} = \dfrac{b_{i} - \displaystyle\sum_{j < i} a_{ij}x_{j}^{\left(k+1\right)} - \displaystyle\sum_{j > i}a_{ij}x_{j}^{\left(k\right)}}{a_{ii}}
\end{equation}
At iteration $\left(k+1\right)$, let's consider the computation of $x_{i}^{\left(k+1\right)}$. we observer that for $j < i$ (with $i \ge 2$), $x_{j}^{\left(k+1\right)}$ is known (we have already calculated it). We can therefore think of using the quantities at step $\left(k+1\right)$ if $j<i$ and, as in the Jacobi method, those at the previous step $k$ if $j > i$.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{tools} \textbf{Algorithm}}
\end{flushleft}
\begin{enumerate}
    \item \textbf{Start with an initial guess} $x^{\left(0\right)}$, also zero.
    \item \textbf{Iteration}. For each row $i$ from $1$ to $n$ calculate the value of the equation \ref{eq: gauss seidel x calcolus}.
    \item \textbf{Repeat until the changes are less than a specified tolerance}.
\end{enumerate}

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{dollar-sign} \textbf{How much does it cost?}}
\end{flushleft}
The cost is comparable to the Jacobi method explained on page \pageref{general-ref: cost jacobi method}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{network-wired} \textbf{Can it be parallelized?}}
\end{flushleft}
Unlike the Jacobi method, the Gauss-Seidel method relies on the most recent updates within the same iteration. This sequential dependency \textbf{makes it more difficult to parallelize, as each update depends on the previous ones}.

\highspace
While it's harder to parallelize due to its inherent sequential nature, we can still achieve some degree of parallelism with clever strategies such as red-black ordering. This makes the Gauss-Seidel method less straightforward to parallelize than Jacobi, but not impossible.

\newpage

\subsubsection{Convergence of Jacobi and Gauss-Seidel methods}

Let be a general matrix $A$, and :
\begin{itemize}
    \item $D$ the \textbf{diagonal part} of $A$
    \item $-E$ \textbf{lower triangular part} of $A$
    \item $-F$ \textbf{upper triangular part} of $A$
\end{itemize}
\begin{equation*}
    A = \begin{bmatrix}
        & & & & \\
        & \ddots &   & -F     & \\
        &        & D &        & \\
        & -E     &   & \ddots & \\
        & & & &
    \end{bmatrix}
\end{equation*}
The previous Jacobi and Gauss-Seidel methods can be rewritten as:
\begin{itemize}
    \item Jacobi:
    \begin{itemize}
        \item Method:
        \begin{equation*}
            D\mathbf{x}^{\left(k+1\right)} = \left(E+F\right)\mathbf{x}^{\left(k\right)} + \mathbf{b}
        \end{equation*}
        \item Iteration matrix:
        \begin{equation*}
            B_{J} = D^{-1}\left(E+F\right) = D^{-1}\left(D-A\right) = I-D^{-1}A
        \end{equation*}
    \end{itemize}

    \item Gauss-Seidel
    \begin{itemize}
        \item Method:
        \begin{equation*}
            \left(D-E\right)\mathbf{x}^{\left(k+1\right)} = F\mathbf{x}^{k} + \mathbf{b}
        \end{equation*}
        \item Iteration matrix:
        \begin{equation*}
            B_{GS} = \left(D-E\right)^{-1}F
        \end{equation*}
    \end{itemize}
\end{itemize}
We present a theorem which gives us the \textbf{sufficient condition for convergence} of the Jacobi and Gauss-Seidel methods.

\begin{theorem}[\textbf{sufficient condition for convergence of Jacobi and Gauss-Seidel}]
    The following conditions are sufficient for convergence:
    \begin{itemize}
        \item If a matrix $A$ is \textbf{strictly diagonally dominant by \underline{rows}}:
        \begin{equation*}
            \left|a_{ii}\right| > \displaystyle\sum_{j \ne i} \left|a_{ij}\right| \hspace{2em} i = 1, \dots, n
        \end{equation*}
        Then Jacobi and Gauss-Seidel converge.

        \item If a matrix $A$ is \textbf{strictly diagonally dominant by \underline{columns}}:
        \begin{equation*}
            \left|a_{ii}\right| > \displaystyle\sum_{j \ne i} \left|a_{ji}\right| \hspace{2em} i = 1, \dots, n
        \end{equation*}
        Then Jacobi and Gauss-Seidel converge.

        \item If a matrix $A$ is SPD (symmetric positive and definite), then the Gauss-Seidel method is convergent.
        
        \item If a matrix $A$ is tridiagonal\footnote{A matrix is \definitionWithSpecificIndex{tridiagonal}{Tridiagonal matrix} when it has non-zero elements only on the main diagonal, the diagonal above the main diagonal, and the diagonal below the main diagonal.
        \begin{equation*}
            A = \begin{bmatrix}
                a_{1,1} & a_{1,2} & 0       & 0  \\
                a_{2,1} & a_{2,2} & a_{2,3} & 0  \\
                0 & a_{3,2} & a_{3,3} & a_{3,4} \\
                0 & 0 & a_{4,3} & a_{4,4} \\
            \end{bmatrix}
        \end{equation*}
        }, then the square spectral value of the Jacobi iteration matrix is equal to the spectral value of the Gauss-Seidel iteration matrix.
        \begin{equation*}
            \rho^{2}\left(B_{J}\right) = \rho\left(B_{GS}\right)
        \end{equation*}
    \end{itemize}
\end{theorem}