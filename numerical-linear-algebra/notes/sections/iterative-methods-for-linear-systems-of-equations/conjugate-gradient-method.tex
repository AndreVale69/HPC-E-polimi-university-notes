\subsection{Conjugate Gradient method}

The \definition{Conjugate Gradient method (GC)} is essentially an iterative algorithm used to solve large linear systems. It is similar to the gradient method, but instead of just following the steepest path, it chooses directions that are conjugate to each other. This avoids backtracking and converges more quickly.

\highspace
\begin{theorem}
    In exact arithmetic the Conjugate Gradient method (GC) converges to the exact solution in at most $n$ iterations. At each iteration $k$, the error $\mathbf{e}^{\left(k\right)} = \mathbf{x} - \mathbf{x}^{\left(k\right)}$ can be bounded by:
    \begin{equation}\label{eq: bound error conjugate gradient}
        {\left|\left|\mathbf{e}^{\left(k\right)}\right|\right|}_{A} \le \dfrac{
            2c^{k}
        }{
            1+c^{2k}
        }
        \cdot
        {\left|\left|\mathbf{e}^{\left(0\right)}\right|\right|}_{A}
    \end{equation}
    With:
    \begin{equation}\label{eq: bound c conjugate gradient}
        c = \dfrac{
            \sqrt{K\left(A\right)} - 1
        }{
            \sqrt{K\left(A\right)} + 1
        }
    \end{equation}
\end{theorem}

\begin{flushleft}
    \textcolor{Green3}{\faIcon{tools} \textbf{Conjugate Gradient Algorithm}}
\end{flushleft}
\begin{enumerate}
    \item \textbf{Start with an \emph{initial guess}} $x^{\left(0\right)}$, an \textbf{\emph{initial residual}} as $\mathbf{r}^{\left(0\right)} = \mathbf{b} - A\mathbf{x}^{\left(0\right)}$, and the \textbf{\emph{initial direction}} $\mathbf{d}^{\left(0\right)} = \mathbf{r}^{\left(0\right)}$.
    \item \textbf{Iteration}. For each $k$ calculate:
    \begin{enumerate}
        \item The parameter $\alpha_{k}$:
        \begin{equation}
            \alpha_{k} = \dfrac{
                \left(\mathbf{d}^{\left(k\right)}\right)^{T}\mathbf{r}^{\left(k\right)}
            }{
                \left(\mathbf{d}^{\left(k\right)}\right)^{T}A\mathbf{d}^{\left(k\right)}
            }
        \end{equation}

        \item The step $k+1$ along the direction $k$:
        \begin{equation}
            \mathbf{x}^{\left(k+1\right)} = \mathbf{x}^{\left(k\right)} + \alpha_{k}\mathbf{d}^{\left(k\right)}
        \end{equation}

        \item The next residual $k+1$:
        \begin{equation}
            \mathbf{r}^{\left(k+1\right)} = \mathbf{r}^{\left(k\right)} - \alpha_{k}A\mathbf{d}^{\left(k\right)}
        \end{equation}

        \item The parameter $\beta_{k}$:
        \begin{equation}
            \beta_{k} = \dfrac{
                \left(A\mathbf{d}^{\left(k\right)}\right)^{T}\mathbf{r}^{\left(k+1\right)}
            }{
                \left(A\mathbf{d}^{\left(k\right)}\right)^{T}\mathbf{d}^{\left(k\right)}
            }
        \end{equation}

        \item The new direction $k+1$:
        \begin{equation}
            \mathbf{d}^{\left(k+1\right)} = \mathbf{r}^{\left(k+1\right)} - \beta_{k}\mathbf{d}^{\left(k\right)}
        \end{equation}
    \end{enumerate}
    \item \textbf{Repeat until the changes are less than a specified tolerance}.
\end{enumerate}
Each new direction is orthogonal (or conjugate) to all previous directions. This orthogonality ensures that each step optimally reduces the error without undoing the progress made in previous steps.

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{tools} \textbf{Preconditioned Conjugate Gradient Algorithm}}
\end{flushleft}
The CG method is modified by introducing $A$ and $P$ as symmetric, positive and definite matrices. The preconditioned system is:
\begin{equation*}
    \underbrace{P^{-1} A P^{-T}}_{\widehat{A}} \underbrace{P^{T} \mathbf{x}}_{\widehat{\mathbf{x}}} = \underbrace{P^{-1}\mathbf{b}}_{\widehat{\mathbf{b}}}
\end{equation*}
\begin{enumerate}
    \item \textbf{Start with an \emph{initial guess}} $x^{\left(0\right)}$, an \textbf{\emph{initial residual}} as $\mathbf{r}^{\left(0\right)} = \mathbf{b} - A\mathbf{x}^{\left(0\right)}$, and the \textbf{\emph{initial direction}} $\mathbf{d}^{\left(0\right)} = \mathbf{r}^{\left(0\right)}$.
    \item \textbf{Iteration}. For each $k$ calculate:
    \begin{enumerate}
        \item The parameter $\alpha_{k}$:
        \begin{equation}
            \alpha_{k} = \dfrac{
                \left(\mathbf{z}^{\left(k\right)}\right)^{T}\mathbf{r}^{\left(k\right)}
            }{
                \left(A\mathbf{d}^{\left(k\right)}\right)^{T}A\mathbf{d}^{\left(k\right)}
            }
        \end{equation}

        \item The step $k+1$ along the direction $k$:
        \begin{equation}
            \mathbf{x}^{\left(k+1\right)} = \mathbf{x}^{\left(k\right)} + \alpha_{k}\mathbf{d}^{\left(k\right)}
        \end{equation}

        \item The next residual $k+1$:
        \begin{equation}
            \mathbf{r}^{\left(k+1\right)} = \mathbf{r}^{\left(k\right)} - \alpha_{k}A\mathbf{d}^{\left(k\right)}
        \end{equation}

        \item Compute the action of the preconditioner $P$ on $\mathbf{r}^{\left(k+1\right)}$:
        \begin{equation}
            P\mathbf{z}^{\left(k+1\right)} = \mathbf{r}^{\left(k+1\right)}
        \end{equation}

        \item The parameter $\beta_{k}$:
        \begin{equation}
            \beta_{k} = \dfrac{
                \left(A\mathbf{d}^{\left(k\right)}\right)^{T}\mathbf{z}^{\left(k+1\right)}
            }{
                \left(A\mathbf{d}^{\left(k\right)}\right)^{T}\mathbf{d}^{\left(k\right)}
            }
        \end{equation}

        \item The new direction $k+1$:
        \begin{equation}
            \mathbf{d}^{\left(k+1\right)} = \mathbf{z}^{\left(k+1\right)} - \beta_{k}\mathbf{d}^{\left(k\right)}
        \end{equation}
    \end{enumerate}
    \item \textbf{Repeat until the changes are less than a specified tolerance}.
\end{enumerate}
With the equations \ref{eq: bound error conjugate gradient} and \ref{eq: bound c conjugate gradient}, the \textbf{preconditioner is considered good if}:
\begin{equation}
    \dfrac{
        \sqrt{K\left(P^{-1}A\right)}-1
    }{
        \sqrt{K\left(P^{-1}A\right)}+1
    }
    <
    \dfrac{
        \sqrt{K\left(A\right)}-1
    }{
        \sqrt{K\left(A\right)}+1
    }
\end{equation}

\newpage

\begin{flushleft}
    \textcolor{Red2}{\faIcon{dollar-sign} \textbf{How much does it cost?}}
\end{flushleft}
The cost of each iteration depends by type of matrix:
\begin{itemize}
    \item \textbf{Dense matrix}: the cost of each iteration is about $n^{2}$ \textbf{operations}.
    \item \textbf{Sparse matrix}: the cost of each iteration is only about $n$ \textbf{operations}.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{network-wired} \textbf{Can it be parallelized?}}
\end{flushleft}
The Conjugate Gradient method has some parts that can be parallelized, such as: matrix-vector products, dot products, and vector updates. However, \textbf{some operations} (such as dot products) \textbf{require global synchronization}, which can \textbf{limit the efficiency of parallelization}. So while we can parallelize parts of it, the method as a whole isn't perfectly parallelizable.
