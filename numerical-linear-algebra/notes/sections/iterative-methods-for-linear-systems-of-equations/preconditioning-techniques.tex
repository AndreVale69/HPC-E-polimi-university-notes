\subsection{Preconditioning techniques}

Preconditioning techniques are used to \textbf{improve the convergence rate} of iterative methods for solving linear systems.

\highspace
The optimal spectral radius $\rho_{\text{opt}}$ (equation \ref{eq: optimal sepctral radius}, page \pageref{eq: optimal sepctral radius}) expresses the maximum convergence speed that can be achieved with a stationary Richardson method. Unfortunately, \textbf{badly conditioned matrices} (where $K\left(A\right) \gg 1$) are characterized by a \textbf{very low convergence rate}. So how can we improve the convergence rate?

\highspace
The main idea is to introduce a symmetric positive definite matrix $P^{-1}$, called a \textbf{preconditioner}. Then the solution of the general problem is equivalent to the following preconditioned system:
\begin{equation}
    A\mathbf{x} = \mathbf{b} \: \equiv \: P^{-\frac{1}{2}} A P^{-\frac{1}{2}} \mathbf{z} = P^{-\frac{1}{2}} \mathbf{b}
\end{equation}
Where $\mathbf{x} = P^{-\frac{1}{2}}\mathbf{z}$. In general, the rule of thumb is to use a $P^{-1}$ such that $K\left(P^{-\frac{1}{2}} A P^{-\frac{1}{2}}\right) \ll K\left(A\right)$.

\highspace
Suppose that $P^{-1}A$ has real and positive eigenvalues. We apply the stationary Richardson method to $P^{-1}A$:
\begin{equation}\label{eq: stationary richardson with preconditioner}
    \mathbf{x}^{\left(k+1\right)} = \mathbf{x}^{\left(k\right)} + \alpha P^{-1}\left(\mathbf{b} - A \mathbf{x}^{\left(k\right)}\right) = \mathbf{x}^{\left(k\right)} + \alpha P^{-1}\mathbf{r}^{\left(k\right)}
\end{equation}
We obtain the same convergence results as in the non-preconditioned case, provided we replace $A$ with $P^{-1}A$:
\begin{itemize}
    \item \textbf{Preconditioned convergence}:
    \begin{equation}
        0 < \alpha < \dfrac{2}{\lambda_{\max}\left(P^{-1}A\right)}
    \end{equation}

    \item \textbf{Preconditioned optimal values}:
    \begin{itemize}
        \item Optimal alpha:
        \begin{equation}
            \alpha_{\text{opt}} = \dfrac{2}{\lambda_{\min}\left(P^{-1}A\right) + \lambda_{\max}\left(P^{-1}A\right)}
        \end{equation}

        \item Optimal spectral radius:
        \begin{equation}
            \rho_{\text{opt}} = \dfrac{
                K\left(P^{-1}A\right)-1
            }{
                K\left(P^{-1}A\right)+1
            }
        \end{equation}
    \end{itemize}
\end{itemize}
Since $K\left(P^{-1}A\right) \ll K\left(A\right)$ we obtain a higher convergence rate, we can conclude that the preconditioner method is faster than the non-preconditioned case? Well, the topic is little more complicated. \textbf{Preconditioning usually makes iterative methods converge faster} because it improves the condition number of the system. However, the effectiveness of preconditioning depends on the specific problem and the preconditioner chosen. In \textbf{some cases}, the \textbf{overhead of applying the preconditioner can offset its benefits}, so while preconditioning generally helps, it's not a guaranteed speedup every time.

\newpage

\subsubsection{Preconditioned Richardson method}

The stationary Richardson method explained on page \pageref{subsubsection: The stationary Richardson method} is the same in this case, but we also choose to apply a preconditioner.

\highspace
Remember that:
\begin{itemize}
    \item The core of the stationary Richardson method defined on page \pageref{eq: stationary richardson with preconditioner} is:
    \begin{equation*}
        \mathbf{x}^{\left(k+1\right)} = \mathbf{x}^{\left(k\right)} + \alpha P^{-1}\left(\mathbf{b} - A \mathbf{x}^{\left(k\right)}\right) = \mathbf{x}^{\left(k\right)} + \alpha P^{-1}\mathbf{r}^{\left(k\right)}
    \end{equation*}

    \item The preconditioned residual:
    \begin{equation*}
        \mathbf{z}^{\left(k\right)} = P^{-1}\mathbf{r}^{\left(k\right)}
    \end{equation*}
\end{itemize}
We define the pseudo-algorithm as follows. For any $k = 0, 1, 2, \dots$:
\begin{enumerate}
    \item \textbf{Compute}
    \begin{equation*}
        \alpha_{\text{opt}} = \dfrac{2}{\lambda_{\min}\left(P^{-1}A\right) + \lambda_{\max}\left(P^{-1}A\right)}
    \end{equation*}

    \item \textbf{Update}
    \begin{equation*}
        \mathbf{r}^{\left(k\right)} = \mathbf{b} - A\mathbf{x}^{\left(k\right)}
    \end{equation*}

    \item \textbf{Solve}
    \begin{equation*}
        P\mathbf{z}^{\left(k\right)} = \mathbf{r}^{\left(k\right)}
    \end{equation*}

    \item \textbf{Update}
    \begin{equation*}
        \mathbf{x}^{\left(k+1\right)} = \mathbf{x}^{\left(k\right)} + \alpha_{\text{opt}} \mathbf{z}^{\left(k\right)}
    \end{equation*}
\end{enumerate}