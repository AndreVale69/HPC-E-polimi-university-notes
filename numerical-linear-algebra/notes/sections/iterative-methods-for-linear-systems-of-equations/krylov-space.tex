\subsection{Krylov-space}

Krylov space methods are a group of iterative techniques used to solve large linear systems or eigenvalue problems. These methods construct a sequence of subspaces, called Krylov subspaces, which are iteratively expanded to approximate the solution.

\begin{definitionbox}[: Krylov (sub)space]
    Given a nonsingular $A \in \mathbb{R}^{n \times n}$ and $\mathbf{y} \in \mathbb{R}^{n}$, $\mathbf{y} \ne \mathbf{0}$, the $k$th Krylov (sub)space $\mathcal{K}_{k}\left(A, \mathbf{y}\right)$ generated by $A$ from $\mathbf{y}$ is:
    \begin{equation}
        \mathcal{K}_{k}\left(A, \mathbf{y}\right) = \mathrm{span}\left(\mathbf{y}, A\mathbf{y}, \dots, A^{k-1}\mathbf{y}\right)
    \end{equation}
    Clearly, it holds:
    \begin{equation*}
        \mathcal{K}_{1}\left(A, \mathbf{y}\right)
        \subseteq
        \mathcal{K}_{2}\left(A, \mathbf{y}\right)
        \subseteq
        \cdots
    \end{equation*}
\end{definitionbox}

\noindent
It seems clever to choose the $k$th approximate solution $\mathbf{x}^{\left(k\right)}$:
\begin{equation*}
    \mathbf{x}^{\left(k\right)} \in \mathbf{x}^{\left(0\right)} + \mathcal{K}_{k}\left(A, \mathbf{r}^{\left(0\right)}\right)
\end{equation*}
But can we expect to find the exact solution $\mathbf{x}$ of $A\mathbf{x} = \mathbf{b}$ in one of those affine space?
\begin{lemma}
    Let $\mathbf{x}$ be the solution of $A\mathbf{x} = \mathbf{b}$ and let $\mathbf{x}^{\left(0\right)}$ be any initial approximation of it and $\mathbf{r}^{\left(0\right)} = \mathbf{b} - A\mathbf{x}^{\left(0\right)}$ the corresponding residual. Moreover, let $v = v\left(\mathbf{r}^{\left(0\right)}, A\right)$ be the so called \textbf{grade of $\mathbf{r}^{\left(0\right)}$ with respect to $A$}. Then:
    \begin{equation*}
        \mathbf{x} \in \mathbf{x}^{\left(0\right)} + \mathcal{K}_{v}\left(A, \mathbf{r}^{\left(0\right)}\right)
    \end{equation*}
\end{lemma}

\begin{lemma}
    There is a positive integer $\nu = \nu\left(\mathbf{r}^{\left(0\right)}, A\right)$ called \textbf{grade of $\mathbf{y}$ with respect to $A$}, such that:
    \begin{equation*}
        \begin{array}{rcl}
            \dim\left(\mathcal{K}_{s}\left(A, y\right)\right) &=& s \text{ if } s \le \nu\\ [.5em]
            \dim\left(\mathcal{K}_{s}\left(A, y\right)\right) &=& \nu \text{ if } s \ge \nu
        \end{array}
    \end{equation*}
    $\mathcal{K}_{\nu}\left(A, y\right)$ is the smallest $A$-invariant subspace that contains $\mathbf{y}$.
\end{lemma}

\begin{lemma}
    The nonnegative integer $\nu = \nu\left(\mathbf{y}, A\right)$ of $\mathbf{y}$ with respect to $A$ satisfies:
    \begin{equation*}
        \nu\left(\mathbf{y}, A\right) = \min\left\{
            s \: \left| \: A^{-1}\mathbf{y} \in \mathcal{K}_{s}\left(A, y\right) \right.
        \right\}
    \end{equation*}
\end{lemma}

\noindent
The idea behind Krylov space solvers is to \textbf{generate a sequence of approximate solutions} $\mathbf{x}^{\left(k\right)} \in \mathbf{x}^{\left(0\right)} + \mathcal{K}_{k}\left(A, \mathbf{r}^{\left(0\right)}\right)$ of $A\mathbf{x} = \mathbf{b}$ so that the corresponding \textbf{residuals} $\mathbf{r}^{\left(k\right)} \in \mathcal{K}_{k+1}\left(A, \mathbf{r}^{\left(0\right)}\right)$ \textbf{\emph{converge} to the zero vector} $\mathbf{0}$.

\highspace
The \emph{converge} may also \textbf{mean that after a finite number of steps}, $\mathbf{r}^{\left(k\right)} = \mathbf{0}$, so that $\mathbf{x}^{\left(k\right)} = \mathbf{x}$ and the process stops. This is especially true (in exact arithmetic) if a \textbf{method ensures that the residuals are linearly independent}: then $\mathbf{r}^{\left(\nu\right)} = \mathbf{0}$. In this case, we say that the \textbf{method has the property of finite termination}.

\begin{definitionbox}[: (standard) Krylov space]
    A (standard) Krylov space method for solving a linear system $A\mathbf{x} = \mathbf{b}$ or, briefly, a Krylov space solver is an iterative method starting from some initial approximation $\mathbf{x}^{\left(0\right)}$ and the corresponding residual $\mathbf{r}^{\left(0\right)}$ and generating for all, or at least most $k$, until it possibly finds the exact solution, iterates $\mathbf{x}^{\left(k\right)}$ such that:
    \begin{equation}
        \mathbf{x}^{\left(k\right)} = \mathbf{x}^{\left(0\right)} + p_{k-1} \left(A\right)\mathbf{r}^{\left(0\right)}
    \end{equation}
    With a polynomial $p_{k-1}\left(A\right)$ of exact degree $k-1$. For some $k$, $\mathbf{x}^{\left(k\right)}$ may not exist or $p_{k-1}\left(A\right)$ may have lower degree.
\end{definitionbox}

\noindent
The conjugate gradient method is a Krylov space solver.