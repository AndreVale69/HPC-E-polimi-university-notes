\subsection{Gradient method}

The Gradient method \textbf{uses the gradient to find the most efficient path to the minimum}. Although the gradient of a function gives the direction to the maximum of a function, if we go the opposite way, we find the minimum. This is the most basic and general idea.

\begin{flushleft}
    \textcolor{Green3}{\faIcon{tools} \textbf{Algorithm}}
\end{flushleft}
\begin{enumerate}
    \item \textbf{Start with an initial guess} $x^{\left(0\right)}$ and an \textbf{initial residual} as $\mathbf{r}^{\left(0\right)} = \mathbf{b} - A\mathbf{x}^{\left(0\right)}$.
    \item \textbf{Iteration}. For each $k$ calculate:
    \begin{enumerate}
        \item The parameter $\alpha_{k}$:
        \begin{equation}
            \alpha_{k} = \dfrac{
                \left(\mathbf{r}^{\left(k\right)}\right)^{T}\mathbf{r}^{\left(k\right)}
            }{
                \left(\mathbf{r}^{\left(k\right)}\right)^{T}A\mathbf{r}^{\left(k\right)}
            }
        \end{equation}

        \item The step $k+1$:
        \begin{equation}
            \mathbf{x}^{\left(k+1\right)} = \mathbf{x}^{\left(k\right)} + \alpha_{k}\mathbf{r}^{\left(k\right)}
        \end{equation}

        \item The next residual:
        \begin{equation}
            \mathbf{r}^{\left(k+1\right)} = \left(I-\alpha_{k}A\right)\mathbf{r}^{\left(k\right)}
        \end{equation}
    \end{enumerate}
    \item \textbf{Repeat until the changes are less than a specified tolerance}.
\end{enumerate}
Where the \textbf{convergence rate} is:
\begin{equation}
    {\left|\left|\mathbf{e}^{\left(k\right)}\right|\right|}_{A} \le \left(\dfrac{K\left(A\right)-1}{K\left(A\right)+1}\right)^{k} \cdot {\left|\left|\mathbf{e}^{\left(0\right)}\right|\right|}_{A}
\end{equation}

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{dollar-sign} \textbf{How much does it cost?}}
\end{flushleft}
The cost of each iteration depends by type of matrix:
\begin{itemize}
    \item \textbf{Dense matrix}: the cost of each iteration is about $n^{2}$ \textbf{operations}.
    \item \textbf{Sparse matrix}: the cost of each iteration is only about $n$ \textbf{operations}.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{network-wired} \textbf{Can it be parallelized?}}
\end{flushleft}
Parallelizing the gradient method involves distributing the computation of gradients and their applications across multiple processors. Then, yes, it is possible.
