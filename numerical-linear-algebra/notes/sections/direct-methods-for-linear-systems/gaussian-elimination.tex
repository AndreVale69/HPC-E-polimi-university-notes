\subsection{Gaussian elimination}

\definition{Gaussian elimination} is a method used to solve systems of linear equations. It involves a \textbf{sequence of operations}:
\begin{itemize}
    \item \textbf{Swapping rows}: reorder the rows to position the pivot elements (non-zero leading coefficients) appropriately.
    \item \textbf{Multiplying a row by a nonzero number}: scale rows to facilitate elimination.
    \item \textbf{Adding a multiple of one row to another row}: use row operations to eliminate variables systematically.
\end{itemize}
The \textbf{goal} is to \textbf{transform the matrix} $A$ \textbf{into an upper triangular matrix} $A^{\left(n\right)} = U$. Besides transforming the matrix $A$, the right vector $\mathbf{b}$ undergoes similar operations to become:
\begin{equation*}
    \begin{array}{rcl}
        A^{\left(1\right)}          &=& A           \rightarrow A^{\left(2\right)}          \rightarrow \cdots \rightarrow A^{\left(k\right)}           \rightarrow A^{\left(k+1\right)}            \rightarrow \cdots \rightarrow A^{\left(n\right)} = U \\ [.8em]
        \mathbf{b}^{\left(1\right)} &=& \mathbf{b}  \rightarrow \mathbf{b}^{\left(2\right)} \rightarrow \cdots \rightarrow \mathbf{b}^{\left(k\right)}  \rightarrow \mathbf{b}^{\left(k+1\right)}   \rightarrow \cdots \rightarrow \mathbf{b}^{\left(n\right)} = \mathbf{y}
    \end{array}
\end{equation*}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{tools} \textbf{Algorithm}}
\end{flushleft}
The \textbf{main goal} is to transform the matrix $A$ into an upper triangular form $U$ while transforming the constant vector $b$.
\begin{enumerate}
    \item For each \textbf{pivot position} $k$ from $1$ to $n-1$: $k = 1, \dots, n-1$;

    \item For each \textbf{row} $i$ below pivot position $k$: $i = k+1, \dots, n$;

    \item \textbf{Compute Multiplier} $l_{ik}$:
    \begin{equation*}
        l_{ik} = \dfrac{a_{ik}^{\left(k\right)}}{a_{kk}^{\left(k\right)}}
    \end{equation*}
    Ensure the pivot element $a_{kk}$ is non-zero (swap rows if necessary).

    \item For each \textbf{column} $j$ to the right of the pivot element: $j = k+1, \dots, n$;
    
    \item \textbf{Update Matrix Elements}:
    \begin{equation*}
        a_{ij}^{\left(k+1\right)} = a_{ij}^{\left(k\right)} - l_{ik} a_{kj}^{\left(k\right)}
    \end{equation*}

    \item \textbf{Update the Constants Vector}:
    \begin{equation*}
        b_{i}^{\left(k+1\right)} = b_{i}^{\left(k\right)} - l_{ik} b_{k}^{\left(k\right)}
    \end{equation*}
\end{enumerate}
After completing $n$ steps, the matrix $A$ is transformed into an upper triangular matrix $U$, and we get the transformed vector $y$:
\begin{equation*}
    A^{(n)} = U \hspace{2em} l_{ij} \rightarrow L \hspace{2em} b^{(n)} = y
\end{equation*}

\newpage

\begin{lstlisting}[mathescape=true, caption={Gaussian Elimination Pseudo-Code}]
For $k = 1, \dots, n-1$

    For $i = k + 1, \dots, n$

        $l_{ik} = \dfrac{a_{ik}^{\left(k\right)}}{a_{kk}^{\left(k\right)}}\label{code: Pivot Selection and Division}$

        For $j = k + 1, \dots, n$

            $a_{ij}^{\left(k+1\right)} = a_{ij}^{\left(k\right)} - l_{ik}a_{kj}^{\left(k\right)} \label{code: Row Operations}$

        $b_{i}^{\left(k+1\right)} = b_{i}^{\left(k\right)} - l_{ik}b_{k}^{\left(k\right)}\label{code: Update the Constants Vector}$
\end{lstlisting}

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{dollar-sign} \textbf{How much does it cost?}}
\end{flushleft}
Gaussian elimination involves several steps, each with different computational requirements. We \textbf{quantify these steps in terms of floating point operations} (flops). For any step $k$ from 1 to $n$:
\begin{itemize}
    \item[Row \ref{code: Pivot Selection and Division}.] \important{Pivot Selection and Division}. This step involves selecting the pivot element and performing the necessary division to normalize the row. This process \textbf{requires} $n - k$ \textbf{flops}.

    \item[Row \ref{code: Row Operations}.] \important{Row Operations}. \textbf{To eliminate the elements below the pivot}, we need $2\left(n - k\right)^{2}$ \textbf{flops} for the necessary multiplications and subtractions.

    \item[Row \ref{code: Update the Constants Vector}] \important{Update the Constants Vector}. The \textbf{total flops} for each step is $2\left(n - k\right)$.
\end{itemize}
\important{Overall Computational Cost}. Summing the costs over all steps (from $k = 1$ to $n - 1$):
\begin{equation*}
    \sum_{k=1}^{n-1} \left(2 \cdot \left(n - k\right)^{2} + 3 \cdot \left(n - k\right)\right) = \sum_{p=1}^{n-1} \left(2 \cdot p^{2} + 3 \cdot p\right)
\end{equation*}
This can be simplified to:
\begin{equation*}
    2 \cdot \dfrac{n \cdot \left(n-1\right)\cdot\left(2 \cdot n-1\right)}{6} + 3 \cdot \frac{n \cdot \left(n-1\right)}{2} \approx \frac{2}{3} n^{3}
\end{equation*}

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{check-circle} \textbf{Sufficient conditions for Gaussian elimination}}
\end{flushleft}
There are \textbf{two sufficient conditions} for Gaussian elimination to successfully reduce a matrix to row echelon form without encountering any issues like division by zero or numerical instability:
\begin{enumerate}
    \item[\textcolor{Green3}{\faIcon{check}}] \textcolor{Green3}{\textbf{Strict Diagonal Dominance}}.
    \begin{itemize}
        \item A matrix $A$ is strictly diagonally dominant by rows if:
        \begin{equation*}
            \left|a_{ii}\right| > \displaystyle\sum_{j \neq i} \left|a_{ij}\right|, \hspace{2em} i = 1, \dots, n
        \end{equation*}

        \item Similarly, $A$ is strictly diagonally dominant by columns if:
        \begin{equation*}
            \left|a_{ii}\right| > \displaystyle\sum_{j \neq i} \left|a_{ji}\right|, \hspace{2em} i = 1, \dots, n
        \end{equation*}
    \end{itemize}
    This condition \textbf{ensures that the pivot elements are sufficiently large}, preventing division by small numbers and minimizing numerical errors.
    
    \item[\textcolor{Green3}{\faIcon{check}}] \textcolor{Green3}{\textbf{Symmetric Positive Definite (SPD) Matrix}}. A matrix $A$ is symmetric positive definite if for all non-zero vectors $z \in \mathbb{R}^n$:
    \begin{equation*}
        \forall z \in \mathbb{R}^{n} \hspace{2em} z \ne 0 \hspace{2em}  z^{T} A z > 0
    \end{equation*}
    This condition \textbf{guarantees that the matrix has positive eigenvalues}, \textbf{ensuring stability during the elimination process} and \textbf{that the matrix is invertible}.
\end{enumerate}