\subsection{Singular Value Decomposition (SVD)}

\definition{Singular Value Decomposition (SVD) method} is a factorization of a matrix into three other matrices. For any $m \times n$ matrix $A$, the SVD is given by:
\begin{equation}\label{eq: singular value decomposition}
    A = U \Sigma V^{T}
\end{equation}
It \textbf{provides a solution to Least Squares techniques}. Where:
\begin{itemize}
    \item $U$ is an $m \times m$ orthogonal matrix, called \textbf{left singular vectors}. These vectors form an \emph{orthonormal basis} for the column space of $A$.

    \item $\Sigma$ is a $m \times n$ diagonal matrix with non-negative real numbers on the diagonal, called \textbf{singular values}. These values are sorted in \textbf{descending order} (from largest to smallest), and the number of values is guaranteed by the minimum between the number of columns and the number of rows; if $A$ is $m \times n$, there are $\min\left(m,n\right)$ singular values.
    
    These values are important because keeping only the largest singular values can reduce the dimensions of the data while preserving important features. It also compresses the image, if the matrix represents an image, and filters out noise.

    \item $V$ is an $n \times n$ orthogonal matrix, called \textbf{right singular vectors}. These vectors form an orthonormal basis for the row space of $A$.
\end{itemize}

\begin{theorem}
    Let $A \in \mathbb{R}^{m \times n}$. There exist two orthogonal matrices $U \in \mathbb{R}^{m \times m}$ and $V \in \mathbb{R}^{n \times n}$ such that:
    \begin{equation}\label{eq: SVD theorem}
        U^{T} A V = \Sigma = \mathrm{diag}\left(\sigma_{1}, \dots, \sigma_{p}\right) \in \mathbb{R}^{m \times n}
    \end{equation}
    With $p = \min\left(m, n\right)$ and $\sigma_{1} \ge \dots \ge \sigma_{p} \ge 0$.
\end{theorem}

\noindent
This method is a robust mathematical tool commonly employed in machine learning for tasks such as dimensionality reduction, data compression and feature extraction. It is especially effective in handling high-dimensional datasets, helping to lower computational complexity and enhance the efficiency of machine learning algorithms.
\begin{itemize}
    \item[\ding{51}] Singular Value Decomposition (SVD) is an \textbf{alternative to Eigenvalue Decomposition}, which is \textbf{generally better} for rank-deficient and ill-conditioned matrices.
    \item[\ding{51}] Computing the \textbf{SVD is always numerically stable} for any matrix but is typically \textbf{more expensive than other decompositions}.
    \item[\ding{51}] SVD can be used to \textbf{compute low-rank approximations} to a matrix via Principal Component Analysis (PCA has many practical applications, and usually large sparse matrices arise).
\end{itemize}

\newpage
\begin{flushleft}
    \textcolor{Green3}{\faIcon{square-root-alt} \textbf{SVD features}}
\end{flushleft}
\begin{itemize}
    \item If $A$ is a real-valued matrix, $U$ and $V$ will also be real-valued and in the equation \ref{eq: SVD theorem}, $U^{T}$ must be written instead of $U^{H}$.

    \item The singular values holds:
    \begin{equation}
        \sigma_{i}\left(A\right) = \sqrt{\lambda_{i}\left(A^{T} A\right)} \hspace{2em} i = 1, \dots, p
    \end{equation}

    \item Since $AA^{T}$ and $A^{T}A$ are symmetric matrices, the columns of $U$ turn out to be the eigenvectors of $A^{T}A$ and, therefore, they are not uniquely defined. The same holds for the columns of $V$, which are the right singular vectors of $A$.

    \item As far as the $\mathrm{rank}\left(A\right)$ is concerned, if:
    \begin{equation*}
        \sigma_{1} \ge \sigma_{2} \ge \dots \ge \sigma_{r} > 0 \hspace{2em} \text{and} \hspace{2em} \sigma_{r+1} = \dots = \sigma_{p} = 0
    \end{equation*}
    Then the rank of $A$ is $r$, the kernel of $A$ is the span of the column vectors of $V$, $\left\{\mathbf{v}_{r+1}, \dots, \mathbf{v}_{n}\right\}$, and the range of $A$ is the span of the column vectors of $U$, $\left\{\mathbf{u}_{1}, \dots, \mathbf{u}_{p}\right\}$.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{bookmark} \textbf{Generalized inverse}}
\end{flushleft}
The \definition{Generalized Inverse of a matrix} $A$ is a matrix that can provide \textbf{solutions to systems of linear equations} that may not have unique solutions or may not be solvable using the regular inverse (such as least squares problems). There are different types of generalized inverses, but one of the most commonly used is the \textbf{Moore-Penrose pseudo-inverse}, denoted as $A^{\dagger}$.

\begin{definitionbox}[: Moore-Penrose]
    Suppose that $A \in \mathbb{R}^{m \times n}$ has rank equal to $r$ and that it admits a SVD of the type $U^{T} A V = \Sigma$. The matrix:
    \begin{equation}
        A^{\dagger} = V \Sigma^{\dagger} U^{T}
    \end{equation}
    Is called the \definition{Moore-Penrose pseudo-inverse matrix}, being:
    \begin{equation}
        \Sigma^{\dagger} = \mathrm{diag}\left\{
            \dfrac{1}{\sigma_{1}},
            \dots,
            \dfrac{1}{\sigma_{p}},
            0,
            \dots,
            0
        \right\}
    \end{equation}
    The matrix $A^{\dagger}$ is also called the \textbf{generalized inverse of $A$}. Also, if $n = m = \mathrm{rank}\left(A\right)$, then $A^{\dagger} = A^{-1}$.
\end{definitionbox}

\noindent
The Moore-Penrose pseudo-inverse matrix is used in the SVD method to \textbf{solve the overdetermined systems} using the least squares technique.

\begin{theorem}
    Let $A \in \mathbb{R}^{m \times n}$ with SVD given by $A = U \Sigma V^{T}$. Then the unique solution to the equation \ref{eq: minimal Euclidean norm} is:
    \begin{equation}
        \mathbf{x}^{*} = A^{\dagger}\mathbf{b}
    \end{equation}
    Where $A^{\dagger}$ is the pseudo-inverse of $A$.
\end{theorem}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{tools} \textbf{How to calculate SVD}}
\end{flushleft}
The \definition{Householder reflection} (or Householder transformation) is a method used in linear algebra to zero out the subdiagonal elements of a matrix, transforming it into a simpler form such as an upper triangular matrix or a bidiagonal matrix. 

\highspace
The use of Householder reflections is \textbf{recommended} because they provide a \textbf{numerically stable and efficient way to reduce a matrix to bidiagonal form}. This reduction makes the subsequent \textbf{steps of the SVD calculation easier and more computationally efficient}.
\begin{equation*}
    U_{1}^{T}AV_{1} = B = \begin{bmatrix}
        d_{1} & f_{1} & \cdots & \cdots & 0_{n} \\
        0 & d_{2} & f_{2} & \cdots & \vdots \\
        \vdots & \vdots & \ddots & \ddots & \vdots \\
        \vdots & \vdots & \vdots & d_{n-1} & f_{n-1} \\
        0 & 0 & \cdots & 0 & d_{n}
    \end{bmatrix}
\end{equation*}
It follows that $T = B^{T}B$ is symmetric and tridiagonal. We could then apply the QR algorithm directly to $B$.