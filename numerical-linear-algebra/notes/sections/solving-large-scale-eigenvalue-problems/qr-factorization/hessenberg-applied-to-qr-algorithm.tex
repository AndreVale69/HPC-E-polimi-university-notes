\subsubsection{Hessenberg applied to QR algorithm}

A matrix $H \in \mathbb{C}^{n \times n}$ is called a \definition{Hessenberg matrix} if its elements below the lower off-diagonal are zero:
\begin{equation*}
    h_{ij} = 0 \hspace{2em} i > j + 1
\end{equation*}
For example:
\begin{equation*}
    H = \begin{bmatrix}
        * & * & * & * & * & * \\
        * & * & * & * & * & * \\
        0 & * & * & * & * & * \\
        0 & 0 & * & * & * & * \\
        0 & 0 & 0 & * & * & * \\
        0 & 0 & 0 & 0 & * & *
    \end{bmatrix}
\end{equation*}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Why do we use Hessenberg?}}
\end{flushleft}
Apply the QR method to a Hessenberg matrix can be decrease the number of operations from $n^{3}$ (Schur decomposition, page \pageref{subsubsection: Schur decomposition applied to QR algorithm}) to $n^{2}$ \textbf{operations}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{tools} \textbf{Algorithm}}
\end{flushleft}
\textbf{Goal}: compute a Hessenberg matrix $H$ and an orthogonal matrix $U$ such that $A = UHU^{H}$ is the QR decomposition of $A$. Such  a reduction can be done with a finite number of operations.
\begin{enumerate}
    \item \textbf{Initial Transformation to Hessenberg Form}. Take as input the matrix $A$, we convert $A$ to a Hessenberg matrix $H$ using similarity transformations techniques.
    
    \item \textbf{Initial guess and initial accumulation of orthogonal transformations}. The first guess is the first Hessenberg form we got from the previous step, and for the $U^{\left(0\right)}$ we take the identity as always:
    \begin{equation*}
        \begin{array}{rcl}
            H^{\left(0\right)} &=& H \\ [.3em]
            U^{\left(0\right)} &=& I
        \end{array}
    \end{equation*}

    \item \textbf{Iteration}. For each $k \ge 1$:
    \begin{enumerate}
        \item \textbf{Hessenberg QR Decomposition}. Decompose the matrix $H^{\left(k-1\right)}$ into the product of an orthogonal matrix $Q^{\left(k\right)}$ and an upper triangular matrix $R^{\left(k\right)}$:
        \begin{equation*}
            H^{\left(k-1\right)} = Q^{\left(k\right)} R^{\left(k\right)}
        \end{equation*}

        \item \textbf{Update the Hessenberg matrix $H$} to be used in next iteration by multiplying $R^{\left(k\right)}$ and $Q^{\left(k\right)}$:
        \begin{equation*}
            H^{\left(k\right)} = R^{\left(k\right)} Q^{\left(k\right)}
        \end{equation*}

        \item \textbf{Update the Transformations matrix $U$} to keep track of the cumulative orthogonal transformations:
        \begin{equation*}
            U^{\left(k\right)} = U^{\left(k-1\right)} Q^{\left(k\right)}
        \end{equation*}
    \end{enumerate}

    \item \textbf{Repeat until we meet a specific stopping criteria}.
    
    \item \textbf{Results}. If a certain stopping criterion is met, we have the upper triangular matrix $H^{\left(k\right)}$ and the orthogonal matrix $U^{\left(k\right)}$. The Schur decomposition using the Hessenberg matrix gives us an important result:
    \begin{equation*}
        H = H^{\left(k\right)} \: \land \: U^{\left(k\right)} = U \: \Longrightarrow \: A = U H U^{H} \: \equiv \: U^{H} A U = H
    \end{equation*}
    In other words, in the end we get:
    \begin{itemize}
        \item The \textbf{unitary matrix} $U$ ($U^{H}U = I$), where the \textbf{columns are the orthonormal \underline{eigenvectors}} of the original matrix $A$.
        
        \item The \textbf{upper triangular matrix} $H$, where the elements of the \textbf{diagonal are the \underline{eigenvalues}} of the original matrix $A$.
    \end{itemize}
\end{enumerate}

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{dollar-sign} \textbf{How much does it cost?}}
\end{flushleft}
As we have already said, the Hessenberg matrix \textbf{reduces the computational cost to} $n^{2}$, which is more competitive than the Schur decomposition ($n^{3}$).

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{network-wired} \textbf{Can it be parallelized?}}
\end{flushleft}
As we have seen with the other QR methods, parallelization is still \textbf{difficult}. It can be achieved with some very optimized libraries, but in general it is complicated due to its dependencies.