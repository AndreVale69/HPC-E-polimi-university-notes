\section{Solving large scale eigenvalue problems}

\subsection{Eigenvalue problems}

Eigenvalue problems involve \textbf{finding scalar values (eigenvalues) and corresponding vectors (eigenvectors) that satisfy the equation} $A\mathbf{x} = \lambda \mathbf{x}$, where $A$ is a square matrix, $x$ is the eigenvector, and $\lambda$ is the eigenvalue.

\highspace
Mathematically, the algebraic eigenvalue problem reads as follows. Given a matrix $A \in \mathbb{C}^{n \times n}$, find $\left(\lambda, \mathbf{v}\right) \in \mathbb{C} \times \mathbb{C}^{n} \setminus \left\{\mathbf{0}\right\}$ such that:
\begin{equation}\label{eq: eigenvalue problem equation}
    A\mathbf{v} = \lambda\mathbf{v}
\end{equation}
Where:
\begin{itemize}
    \item $\lambda$ is an eigenvalue of $A$
    \item $\mathbf{v}$ (non-zero) is the corresponding eigenvector
\end{itemize}
Thus, equation \ref{eq: eigenvalue problem equation} represents the \textbf{equation that must be satisfied to solve the eigenvalue problem}. Some features:
\begin{itemize}
    \item The \textbf{set of all the eigenvalues} of a matrix $A$ is called the \definitionWithSpecificIndex{spectrum}{Spectrum of a matrix} of $A$ and is represented as $\sigma\left(A\right)$
    \item The \textbf{maximum modulus of all the eigenvalues} is called the \definitionWithSpecificIndex{spectral radius}{Spectral radius of a matrix} of $A$:
    \begin{equation}
        \rho\left(A\right) = \max\left\{\left|\lambda\right| \: : \: \lambda \in \lambda\left(A\right)\right\}
    \end{equation}
\end{itemize}

\begin{flushleft}
    \textcolor{Green3}{\faIcon{square-root-alt} \textbf{Mathematical background}}
\end{flushleft}
Here is a list of some mathematical concepts that are useful for studying the following chapter.
\begin{itemize}
    \item The problem (equation \ref{eq: eigenvalue problem equation}) $A\mathbf{v} = \lambda \mathbf{v}$ is equivalent to $\left(A - \lambda I\right) \mathbf{v} = 0$.
    
    \item The equation \ref{eq: eigenvalue problem equation} has a nonzero solution $\mathbf{v}$ if and only if its matrix is singular, that is the eigenvalues of $A$ are the values $\lambda$ such that $\det\left(A - \lambda I\right) = 0$.

    \item The $\det\left(A - \lambda I\right) = 0$ is a polynomial of degree $n$ in $\lambda$. It is called the \textbf{characteristic polynomial} of $A$ and its roots are the eigenvalues of $A$.
    
    \item From the Fundamental Theorem of Algebra, an $n \times n$ matrix $A$ always has $n$ eigenvalues $\lambda_{i}$, $i = 1, \dots, n$.

    \item Each $\lambda_{i}$ may be real but in general is a complex number.
    
    \item The eigenvalues $\lambda_{1}, \lambda_{2}, \dots, \lambda_{n}$ may not all have distinct values.
    
    \item Rayleigh quotient: let $\left(\lambda_{i}, \mathbf{v}_{i}\right)$ be an eigenpair of $A$, then:
    \begin{equation*}
        \lambda_{i} = \dfrac{
            \mathbf{v}_{i}^{H} A \mathbf{v}_{i}
        }{
            \mathbf{v}_{i}^{H} \mathbf{v}_{i}
        }
    \end{equation*}
\end{itemize}

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{tachometer-alt} \textbf{Similarity transformations to simplify eigenvalue problems}}
\end{flushleft}
Similarity transformations are crucial in eigenvalue problems because they simplify matrices, making it easier to find eigenvalues. Of course, they don't change the fundamental nature of the original matrix.

\begin{definitionbox}[: Similar matrices]
    The matrix $B$ is \textbf{similar} to the matrix $A$ if there exists a nonsingular matrix $T$ such that $B = T^{-1} A T$. Note that a matrix is nonsingular if there exists another matrix $C$ such that $TC = CT = I$.
\end{definitionbox}

\begin{proof}
    The above definition is indeed true:
    \begin{equation*}
        \begin{array}{ll}
            & B\mathbf{y} = \lambda\mathbf{y} \\ [.3em]
            \Longrightarrow & T^{-1} A T \mathbf{y} = \lambda\mathbf{y} \\ [.3em]
            \Longrightarrow & A \left(T \mathbf{y}\right) = \lambda\left(T \mathbf{y}\right)
        \end{array}
    \end{equation*}
    So that $A$ and $B$ have the same eigenvalues, and if $\mathbf{y}$ is an eigenvector of $B$, then $\mathbf{v} = T\mathbf{y}$ is an eigenvector of $A$.
\end{proof}

\noindent
A square matrix $A$ is called \definitionWithSpecificIndex{diagonalizable}{Diagonalizable matrix} if it is similar to a diagonal matrix.

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{Similarity transformations limitations}}
\end{flushleft}
The \textbf{similarity transformations preserve only the eigenvalues but not the eigenvectors}. This is not so bad because they can be easily recovered.

\highspace
Furthermore, the eigenvalue problems using the similarity transformation are simplified when we use diagonal matrices. Unfortunately, \textbf{some matrices cannot be transformed into diagonal form by a similarity transformation}.

\highspace
However, the similarity transformation is only a small tool. In the following pages, we present three powerful methods that attempt to simplify the eigenvalue problem.