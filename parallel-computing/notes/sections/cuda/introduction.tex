\section{CUDA}

Although the CUDA topic has already been discussed in section \ref{subsubsection: Basics of CUDA} (page \pageref{subsubsection: Basics of CUDA}), here we introduce more technical topics, a kind of CUDA laboratory. It is done because in the Parallel Computing course we have done a CUDA theory part and a CUDA practice part.

\subsection{Introduction}

\definition{CUDA}, which stands for \textbf{Compute Unified Device Architecture}, is a \textbf{parallel computing platform and application programming interface (API) model created by NVIDIA}. It allows developers to use the power of GPUs (Graphics Processing Units) for general-purpose processing, which enables substantial performance improvements for computationally intensive tasks.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Why CUDA?}}
\end{flushleft}
GPUs, originally designed to render graphics, have evolved into \textbf{highly efficient and powerful processors capable of handling thousands of threads simultaneously}. This transformation has made GPUs, and by extension CUDA, incredibly valuable for applications requiring massive parallelism, such as scientific simulations, machine learning, and deep learning.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Why can we not just use the CPU?}}
\end{flushleft}
Understanding the fundamental differences between CPU and GPU architectures is key to appreciating CUDA's advantages:
\begin{itemize}
    \item CPU (Central Processing Unit):
    \begin{itemize}
        \item Designed for sequential processing.
        \item Features powerful Arithmetic Logic Units (ALUs) with low latency.
        \item Utilizes large hierarchical caches to optimize access to frequently used data.
        \item Employs advanced control mechanisms, such as branch prediction and data forwarding, to minimize delays.
    \end{itemize}

    \item GPU (Graphics Processing Unit):
    \begin{itemize}
        \item Optimized for parallel processing.
        \item Contains a large number of simpler, pipelined ALUs designed for high-throughput computations, despite having longer latency.
        \item Relies on smaller caches to facilitate high memory throughput.
        \item Uses simpler control mechanisms, enabling efficient context switching and handling many threads concurrently.
    \end{itemize}
\end{itemize}
CUDA leverages these GPU characteristics to execute programs with a parallel-first approach, breaking down tasks into smaller, manageable pieces and processing them simultaneously. This approach leads to significant speedups compared to traditional CPU-only processing, making CUDA a pivotal technology for high-performance computing.

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{GPGPU programming paradigms}}
\end{flushleft}
CUDA helped bring General Purpose computing on Graphics Processing Units (GPGPU). Initially designed to accelerate rendering and processing of graphics, GPUs have evolved into versatile processors capable of handling a wide range of computational tasks beyond graphics. This paradigm shift has allowed GPUs to be used for general-purpose computing, providing significant performance improvements for applications requiring high parallelism and computational power (the definition and introduction of GPGPU can be found on page \pageref{def: General-Purpose computing on Graphics Processing Units (GPGPU)}).

\highspace
The GPU (called a device) acts as a co-processor for the CPU (host):
\begin{itemize}
    \item CPU (\textbf{host}):
    \begin{itemize}
        \item \emph{General-purpose} processor.
        \item Handles diverse tasks, running an operating system, and executing a sequence of stored instructions.
        \item Efficient for tasks requiring low-latency access to cached data.
    \end{itemize}

    \item GPU (\textbf{device}):
    \begin{itemize}
        \item Specialized for \emph{high-throughput computation}.
        \item Contains a large number of processing elements for parallel tasks.
        \item Relies on dedicated, high-bandwidth memory.
        \item Best for data-parallel tasks, hiding memory latency effectively.
    \end{itemize}
\end{itemize}
The relationship of these two concepts:
\begin{itemize}
    \item Both CPU and GPU have their own memory spaces.
    \item Optimal performance achieved through cooperation.
    \item CPU handles complex logic and serial tasks; GPU manages parallel processing.
\end{itemize}
About the architecture, the differences are:
\begin{itemize}
    \item CPU (\textbf{host}):
    \begin{itemize}
        \item Composed of control units, ALUs, cache, and DRAM.
        \item Efficient for tasks requiring complex control and low-latency data access.
    \end{itemize}

    \item GPU (\textbf{device}):
    \begin{itemize}
        \item Made up of DRAM and a grid of processing units.
        \item High-throughput design for parallel processing tasks.
    \end{itemize}
\end{itemize}

\noindent
\textbf{GPGPU} (General-Purpose computing on Graphics Processing Units) \textbf{programming paradigms are various approaches or models used to leverage the GPU for general-purpose computation}. These paradigms \textbf{allow programmers to write code that runs efficiently on GPUs}. The main paradigms are:
\begin{enumerate}
    \item \textbf{Applications}. End-user programs that take advantage of GPU performance. Designed to take advantage of the GPU's processing power to accelerate performance (such as video processing software, scientific simulations, and machine learning models).

    \item \textbf{Libraries}. Pre-built functions and routines optimized for GPUs. In other words, a collections of pre-written code and routines that are optimized to run on GPUs. Using these libraries, developers can avoid writing low-level GPU code. Examples of libraries are cuBLAS (for linear algebra), cuFFT (for Fast Fourier Transforms), and TensorFlow (for machine learning).

    \item \textbf{Compiler Directives}. Special instructions embedded in the source code that guide the compiler on how to optimize and parallelize the code for GPU execution. OpenACC and OpenMP are commonly used directives.
 
    \item \textbf{Programming Languages}. Languages or extensions of languages specifically designed for GPGPU programming. They provide the syntax and constructs needed to write code that executes on the GPU. Examples are CUDA and OpenCL (Open Computing Language).
\end{enumerate}

\highspace
Given an application code, in general:
\begin{itemize}
    \item \textbf{Serial Parts run on the CPU} (host). These are the parts of the code that need to be executed sequentially and can benefit from the sophisticated control mechanisms of the CPU.

    \item \textbf{Computation-Intensive and Data-Parallel Parts offloaded to the GPU} (device). These parts can be executed in parallel, making use of the GPU's many cores for faster computation.
\end{itemize}

\highspace
The flow inside the GPU can be described in three general steps:
\begin{enumerate}
    \item \textbf{Copy Input Data from CPU Memory to GPU Memory}. The data is transferred over the PCI bus. This step is critical to prepare the data so that the GPU can access and process it.

    \item \textbf{Load GPU Program and Execute, Caching Data on Chip for Performance}. The GPU program (kernel) is loaded and executed. Data is cached on the GPU chip to improve performance during execution. This step takes advantage of the parallel processing power of the GPU.
    
    \item \textbf{Copy Results from GPU Memory to CPU Memory}. After calculation, the results are transferred back to the CPU memory via the PCI bus. This step is necessary to integrate the results into the larger application or for further processing on the CPU.
\end{enumerate}

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Common GPGPU issues and solutions}}
\end{flushleft}
\begin{itemize}
    \item Data Movement between CPU and GPU is the main Bottleneck.
    \begin{itemize}
        \item \textbf{Low Bandwidth}. Data transfers between the CPU and GPU are relatively slow because they use the PCI Express (PCIe) bus, which has a bandwidth of about 12-14GB/s. This is much lower compared to the internal memory bandwidth of the CPU and GPU.

        \item \textbf{Relatively High Latency}. Data transfer times can be significant, introducing delays that affect overall performance.

        \item \textbf{Data Transfer can take more time than the actual computation}. In some cases, moving data between the CPU and GPU can take longer than the time it takes to perform the actual computations on the GPU. This is a critical point to consider when designing GPU-accelerated applications.
    \end{itemize}

    \item Issues Porting CPU Applications to GPGPU.
    \begin{itemize}
        \item \textbf{Ignoring Data Movement can destroy GPU Performance Benefits}. When converting CPU applications to run on a GPU, it's crucial to manage data transfers efficiently. Failing to do so can negate the performance advantages of using a GPU.

        \item \textbf{Solutions to Hide/Automate Data Transfer}. Some programming techniques and tools can help hide or automate the data transfer process. These solutions can optimize performance by reducing the manual overhead of managing data movement.
    \end{itemize}
\end{itemize}
In other words, efficient data management is the key to performance. In fact, the performance of GPGPU applications depends heavily on how well data is managed between the CPU and the GPU. Efficient data transfer strategies are essential to unleash the full computing power of GPUs.
