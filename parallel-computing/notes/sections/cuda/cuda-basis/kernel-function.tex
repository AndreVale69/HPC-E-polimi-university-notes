\subsubsection{CUDA Kernel}

Launching a CUDA kernel involves \textbf{defining a function that will run on the GPU and then executing this function from the host (CPU) code}.

\begin{enumerate}
    \item \important{Defining functions}.
    \begin{itemize}
        \item CPU Function:
\begin{lstlisting}[language=C++]
void CPUFunction() {
    printf("This function is defined to run on the CPU.");
}\end{lstlisting}
        This function runs on the CPU and prints a message.

        \item GPU Function:
\begin{lstlisting}[language=C++]
__global__ void GPUFunction() {
    printf("This function is defined to run on the GPU.");
}\end{lstlisting}
        This function is defined to run on the GPU, indicated by the\break \texttt{\_\_global\_\_} qualifier, and it prints a message when executed.
    \end{itemize}


    \item \important{Launching the Kernel}. In the main function, we launch the GPU function using a special syntax:
    \begin{lstlisting}[language=C++]
int main() {
    CPUFunction();

    // Launch the GPU kernel
    // with 1 block and 1 thread
    GPUFunction<<<1, 1>>>();
    cudaDeviceSynchronize();
}
    \end{lstlisting}
    \begin{itemize}
        \item The \texttt{CPUFunction()} is called normally, as it's a CPU function.
        \item The \texttt{GPUFunction<<<1, 1>>>();} syntax launches the GPU kernel with a specified execution configuration: \texttt{<<<1, 1>>>} means 1 block and 1 thread per block.
        \item \texttt{cudaDeviceSynchronize()} is called to ensure that the CPU waits for the GPU to finish executing the kernel before continuing.
    \end{itemize}
\end{enumerate}
Just like with variables (table \ref{table: CUDA Memory Types, Scope, and Lifetime}, page \pageref{table: CUDA Memory Types, Scope, and Lifetime}), CUDA provides specific keywords that define the scope and lifetime of functions.
\begin{table}[!htp]
    \centering
    \begin{tabular}{@{} l c c @{}}
        \toprule
        \textbf{Function Declaration} & \textbf{Executed on} & \textbf{Callable from} \\
        \midrule
        \texttt{\_\_device\_\_ type DeviceFunc()} & Device & Device \\
        \texttt{\_\_global\_\_ void KernelFunc()} & Device & Host \\
        \texttt{\_\_host\_\_ type HostFunc()} & Host & Host \\
        \bottomrule
    \end{tabular}
    \caption{CUDA function qualifiers.}
\end{table}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{How can we customize the kernel?}}
\end{flushleft}
Kernel configuration in CUDA involves specifying \textbf{how many blocks and threads will be used} to execute a kernel on the GPU. The key parameters are:
\begin{itemize}
    \item \important{Number of Blocks}. Specifies \textbf{how many blocks} of threads will be launched on the GPU.
    \item \important{Number of Threads per Block}. Specifies \textbf{how many threads} will be \textbf{in each block}.
\end{itemize}
For a \emph{1D hierarchy}, we use simple integer numbers to specify these values because a 1D grid or block can be represented by a single dimension:
\begin{lstlisting}[language=C++]
// Example of launching a kernel with 1 block and 256 threads in 1D
KernelFunc<<<1, 256>>>();    
\end{lstlisting}
For \emph{higher dimensions} (2D or 3D), CUDA uses the \texttt{dim3} \textbf{type to represent the grid and block dimensions}. \texttt{dim3} is a CUDA-specific type that can hold three unsigned integer values, representing the dimensions in the $x$, $y$, and $z$ directions. This provides a flexible way to configure more complex thread hierarchies:
\begin{lstlisting}[language=C++]
// Example of launching a kernel with a 2D grid and blocks
dim3 gridDim(16, 16); // 16 blocks in x and y dimensions
dim3 blockDim(16, 16); // 16 threads per block in x and y dimensions
KernelFunc<<<gridDim, blockDim>>>();
\end{lstlisting}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{tachometer-alt} \textbf{CPU-GPU Synchronization}}
\end{flushleft}
In CUDA, \textbf{kernel executions are asynchronous}, meaning that the \textbf{CPU can continue to execute other instructions while the GPU is running the kernel}. However, there are times when synchronization is required to ensure that the CPU is waiting for the GPU to complete its tasks.
\begin{itemize}
    \item \textbf{Asynchronous Kernel Launch}. When a kernel is launched, the CPU can proceed with subsequent instructions without waiting for the GPU to finish.
\begin{lstlisting}[language=C++]
KernelFunc<<<1, 256>>>();
// The CPU immediately continues executing the next instructions
\end{lstlisting}

    \item \textbf{Synchronizing CPU and GPU}. To synchronize, CUDA provides runtime functions such as \texttt{cudaDeviceSynchronize()}, which ensures that the CPU waits until the GPU has completed all preceding tasks.
\begin{lstlisting}[language=C++]
KernelFunc<<<1, 256>>>();
// Wait for the GPU to finish
cudaDeviceSynchronize();
\end{lstlisting}
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{Error handling}}
\end{flushleft}
The errors in CUDA can be caught in two ways:
\begin{itemize}
    \item Return type: most \textbf{CUDA runtime functions return a variable of type} \texttt{cudaError\_t}. This type is an enumerated value that indicates the success or failure of the function call.
    
    \item Kernels return \texttt{void}: CUDA kernel functions return \texttt{void}, meaning they do not provide direct feedback about errors. Instead, \textbf{errors are checked using specific functions}, such as: \texttt{cudaGetLastError()}. It returns the \textbf{last error that occurred during the execution of a CUDA function}.
    \begin{lstlisting}[language=C++]
cudaError_t err = cudaGetLastError();
if (err != cudaSuccess) {
    printf("CUDA error: %s\n", cudaGetErrorString(err));
}
    \end{lstlisting}
\end{itemize}
\textbf{Asynchronous errors} (during kernel execution) are reported as error states of \texttt{cudaDeviceSynchronize()}. It \textbf{blocks the CPU thread until the GPU has completed all previously requested tasks}. This includes kernel executions, memory transfers, and other asynchronous operations issued.
\begin{lstlisting}[language=C++]
kernel<<<blocks, threads>>>();
cudaError_t err = cudaDeviceSynchronize();
if (err != cudaSuccess) {
    printf("CUDA error: %s\n", cudaGetErrorString(err));
}
\end{lstlisting}
\begin{examplebox}[: error handling]
    \begin{lstlisting}[language=C++]
cudaError_t err1, err2;
err1 = cudaMallocManaged(&a, N);
SomeKernel<<<1, -1>>>(); // Invalid thread number
err2 = cudaGetLastError();
if (err1 != cudaSuccess || err2 != cudaSuccess) {
    printf("Error 1: %s\n", cudaGetErrorString(err1));
    printf("Error 2: %s\n", cudaGetErrorString(err2));
}\end{lstlisting}
    \begin{itemize}
        \item \texttt{err1 = cudaMallocManaged(\&a, N);} attempts to allocate managed memory and returns an error code if it fails.
        \item \texttt{cudaGetErrorString(err1)} converts the error code to a readable string.
        \item \texttt{SomeKernel<<<1, -1>>>();} intentionally uses an invalid thread configuration to generate an error.
        \item \texttt{err2 = cudaGetLastError();} captures the error from the kernel launch.
        \item \texttt{cudaGetErrorString(err2)} converts the kernel launch error code to a readable string.
    \end{itemize}
\end{examplebox}