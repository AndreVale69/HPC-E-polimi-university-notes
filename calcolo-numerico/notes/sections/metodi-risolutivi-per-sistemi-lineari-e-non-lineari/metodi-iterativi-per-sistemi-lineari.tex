\subsection{Metodi iterativi per sistemi lineari}

Un \definition{metodo iterativo} per la risoluzione del sistema lineare $Ax = b$ con:
\begin{itemize}
    \item $A \in \mathbb{R}^{n \times n}$
    \item $b \in \mathbb{R}^{n}$
    \item $x \in \mathbb{R}^{n}$
    \item $\det\left(A\right) \ne 0$
\end{itemize}
Consiste nel costruire una successione di vettori del tipo:
\begin{equation*}
    \left\{\mathbf{x}^{\left(k\right)}, \: k \ge 0\right\}
\end{equation*}
Di $\mathbb{R}^{n}$ che \textbf{converge} alla soluzione esatta $\mathbf{x}$, ossia tale che:
\begin{equation}
    \displaystyle\lim_{k \rightarrow \infty} \mathbf{x}^{\left(k\right)} = \mathbf{x}
\end{equation}
Per un qualunque vettore iniziale $\mathbf{x}^{\left(0\right)} \in \mathbb{R}$, ossia la \textbf{convergenza non deve dipendere dalla scelta di} $\mathbf{x}^{\left(0\right)}$.

\highspace
Inoltre, si definisce ricorsivamente:
\begin{equation}\label{eq: metodi iterativi x con k + 1}
    \mathbf{x}^{\left(k+1\right)} = \mathrm{B}\mathbf{x}^{\left(k\right)} + \mathbf{g} \hspace{2em} k \ge 0
\end{equation}
Essendo:
\begin{itemize}
    \item $\mathrm{B}$ una matrice opportuna (dipendente da $\mathrm{A}$)
    \item $\mathbf{g}$ un vettore opportuno (dipendente da $\mathrm{A}$ e da $\mathbf{b}$)
\end{itemize}

\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{E come si possono scegliere questi valori?}}
\end{flushleft}
La scelta della matrice $\mathrm{B}$ e del vettore $\mathbf{g}$ è piuttosto semplice. Devono essere \textbf{rispettate due proprietà}:
\begin{itemize}
    \item \textbf{Condizione di consistenza}. I valori devono garantire che:
    \begin{equation}\label{eq: condizione di consistenza}
        \mathbf{x} = \mathrm{B}\mathbf{x} + \mathbf{g}
    \end{equation}
    Essendo $\mathbf{x} = \mathrm{A}^{-1}\mathbf{b}$, necessariamente dovrà aversi $\mathbf{g} = \left(\mathrm{I} - \mathrm{B}\right) \mathrm{A}^{-1} \mathbf{b}$. 

    \item \textbf{Condizione di convergenza}. Prima di dare la condizione, è necessario comprendere alcune cose. Prima di tutto, la seguente espressione:
    \begin{equation*}
        \mathbf{e}^{\left(k\right)} = \mathbf{x} - \mathbf{x}^{\left(k\right)}
    \end{equation*}
    Viene identificata come l'\textbf{errore al passo} $k$. Adesso sottraendo l'equazione \ref{eq: metodi iterativi x con k + 1} dalla condizione di consistenza \ref{eq: condizione di consistenza} si ottiene:
    \begin{equation*}
        \mathbf{e}^{\left(k+1\right)} = \mathrm{B}\mathbf{e}^{\left(k\right)}
    \end{equation*}
    Per tale ragione $\mathrm{B}$ viene detta \definition{matrice di iterazione} del metodo \ref{eq: metodi iterativi x con k + 1}.

    E adesso, si presenta la condizione di convergenza:
    \begin{itemize}
        \item La matrice $\mathrm{B}$ è simmetrica\footnote{Quindi corrisponde con la sua trasposta} e definita positiva\footnote{Definizione a pagina: \pageref{matrice definita positiva}}, allora:
        \begin{equation}
            \left|\left| \mathbf{e}^{\left(k+1\right)} \right|\right| 
            = 
            \left|\left| \mathrm{B}\mathbf{e}^{\left(k\right)} \right|\right|
            \le
            \rho\left(\mathrm{B}\right)
            \left|\left| \mathbf{e}^{\left(k\right)} \right|\right|
            \hspace{2em}
            \forall k \ge 0
        \end{equation}
        Dove $\rho\left(\mathrm{B}\right)$ è il \definition{raggio spettrale} di $\mathrm{B}$ ed è il massimo degli autovalori di $\mathrm{B}$. Si tenga conto che nel caso di matrici simmetriche e definite positive esso coincide con il massimo autovalore.

        \item Iterando a ritroso il punto precedente, si ottiene:
        \begin{equation}
            \left|\left| \mathbf{e}^{\left(k\right)} \right|\right| 
            \le
            \left[\rho\left(\mathrm{B}\right)\right]^{k}
            \left|\left| \mathbf{e}^{\left(0\right)} \right|\right| 
            \hspace{2em}
            k \ge 0
        \end{equation}
        Si noti che se $\rho\left(\mathrm{B}\right) < 1$, allora $\mathbf{e}^{\left(k\right)} \rightarrow \mathbf{0}$ per $k \rightarrow \infty$ per ogni possibile $\mathbf{e}^{0}$ (e, conseguentemente, per ogni $\mathbf{x}^{\left(0\right)}$).
    \end{itemize}
\end{itemize}
In generale, la condizione sufficiente e necessaria di convergenza è la seguente.

\begin{definitionbox}[: condizione sufficiente e necessaria di convergenza]
    Un metodo iterativo della forma dell'equazione \ref{eq: metodi iterativi x con k + 1}, la cui matrice di iterazione $B$ soddisfi la condizione di consistenza (eq. \ref{eq: condizione di consistenza}), è \textbf{convergente} per ogni $\mathbf{x}^{\left(0\right)}$ \emph{se e soltanto se} $\rho\left(\mathrm{B}\right) < 1$ (raggio spettrale minore di 1).

    \highspace
    Inoltre, minore è $\rho\left(\mathrm{B}\right)$, minore è il numero di iterazioni necessarie per ridurre l'errore iniziale di un dato fattore.
\end{definitionbox}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Perché introdurre i metodi iterativi?}}
\end{flushleft}
L'esigenza di introdurre i metodi iterativi sorge nel momento in cui si ragiona sulla quantità di tempo spesa da un calcolatore per eseguire la fattorizzazione LU su matrici di grandi dimensioni. Difatti, con matrici con ordini di $10^{7}$, sono necessari circa 11 giorni.

\newpage

\subsubsection{Il metodo di Jacobi}

Se i coefficienti diagonali della matrice $A$ non sono nulli, allora:
\begin{equation*}
    D = \mathrm{diag}\left(a_{11}, a_{22}, \dots, a_{nn}\right)
\end{equation*}
Ovvero $D$ è la matrice diagonale costruita a partire dagli elementi diagonali di $A$. 

\highspace
Il \definition{metodo di Jacobi} corrisponde alla forma:
\begin{equation*}
    \mathrm{D}\mathbf{x}^{\left(k+1\right)} = \mathbf{b} - \left(\mathrm{A} - \mathrm{D}\right)\mathbf{x}^{\left(k\right)} \hspace{2em} k \ge 0
\end{equation*}
Che per componenti assume la forma:
\begin{equation}
    x_{i}^{\left(k+1\right)}
    =
    \dfrac{1}{a_{ii}}
    \left(
        b_{i} - \displaystyle\sum_{j=1, j \ne i}^{n} a_{ij} x_{j}^{\left(k\right)}
    \right)
    \hspace{2em}
    i = 1, \dots, n
\end{equation}
Dove $k \ge 0$ e $\mathbf{x}^{\left(0\right)} = \left(x_{1}^{\left(0\right)}, x_{2}^{\left(0\right)}, \dots, x_{n}^{\left(0\right)}\right)^{T}$ è il vettore iniziale.

\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Quando converge?}}
\end{flushleft}
In due casi:
\begin{enumerate}
    \item Se la matrice $\mathrm{A} \in \mathbb{R}^{n \times n}$ è a dominanza diagonale stressa per righe\footnote{\label{dominanza diagonale stressa per righe}
        In algebra lineare una \definition{matrice a dominanza diagonale stretta per righe} o \definition{matrice diagonale dominante stretta per righe} è una matrice quadrata $A \in \mathbb{C}^{n \times n}$ di ordine $n$ i cui elementi diagonali sono maggiori (non stretta sarebbe maggiori o uguali) in valore assoluto della somma di tutti i restanti elementi della stessa riga in valore assoluto:
        \begin{equation}
            \left| a_{ii} \right| > \displaystyle\sum_{j=1, j \ne i}^{n} \left| a_{ij} \right| 
        \end{equation}
        In senso \underline{non stretto}:
        \begin{equation}
            \left| a_{ii} \right| \ge \displaystyle\sum_{j=1, j \ne i}^{n} \left| a_{ij} \right|
        \end{equation}
    }, allora il metodo di Jacobi converge.

    \item Con la seguente definizione (si veda il paragrafo \ref{subsubsection: il metodo di Gauss-Seidel} per comprendere meglio le definizioni):
    \begin{definitionbox}[: convergenza di Jacobi e Gauss-Seidel]\label{definizione: convergenza di Jacobi e Gauss-Seidel}
        Se $A \in \mathbb{R}^{n \times n}$ è una \definition{matrice tridiagonale}, quindi una matrice quadrata che al di fuori della diagonale principale e delle linee immediatamente al di sopra e al di sotto di essa (la prima sovradiagonale e la prima sottodiagonale), ha solo valori nulli. Un esempio di amtrice tridiagonale:
        \begin{equation*}
            \begin{bmatrix}
                1 & 4 & 0 & 0 \\
                3 & 4 & 1 & 0 \\
                0 & 2 & 3 & 4 \\
                0 & 0 & 1 & 3
            \end{bmatrix}
        \end{equation*}
        Se tale matrice è non singolare, quindi $\det\left(A\right) \ne 0$, con valori della diagonale diversi da zero, allora i metodi di Jacobi e di Gauss-Seidel sono \textbf{entrambi convergenti o entrambi divergenti}.
        \begin{itemize}
            \item Se convergono, la velocità dei metodi:
            \begin{equation*}
                \text{Gauss-Seidel} \gg \text{Jacobi}
            \end{equation*}
            Precisamente il raggio spettrale della matrice di iterazione del metodo di Gauss-Seidel è il quadrato del raggio spettrale di quella del metodo di Jacobi.
        \end{itemize}
    \end{definitionbox}
\end{enumerate}

\highspace
HPC curiosity: è interessante notare che il metodo di Jacobi viene facilmente parallelizzato per aumentare le prestazioni di calcolo.

\newpage

\subsubsection{Il metodo di Gauss-Seidel}\label{subsubsection: il metodo di Gauss-Seidel}

Con il metodo di Jacobi, ogni componente $x_{i}^{\left(k+1\right)}$ del nuovo vettore $\mathbf{x}^{\left(k+1\right)}$ viene calcolata indipendentemente dalle altre. Questa può essere la base di partenza per costruire un nuovo metodo più ottimizzato, poiché se per il calcolo di $x_{i}^{\left(k+1\right)}$ venissero usate le nuove componenti già disponibili $x_{j}^{\left(k+1\right)}$, $j = 1, \dots, i-1$, assieme con le vecchie $x_{j}^{\left(k\right)}$, $j \ge i$.

\highspace
Supponendo che gli elementi della diagonale non siano nulli, per $k \ge 0$, il \definition{metodo di Gauss-Seidel}:
\begin{equation}\label{eq: metodo di Gauss-Seidel}
    x_{i}^{\left(k+1\right)} = 
    \dfrac{1}{a_{ii}} \left(
        b_{i} -
        \displaystyle\sum_{j=1}^{i-1} a_{ij}x_{j}^{\left(k+1\right)} -
        \displaystyle\sum_{j=i+1}^{n} a_{ij}x_{j}^{\left(k\right)}
    \right)
    \hspace{2em}
    i = 1, \dots, n
\end{equation}
HPC curiosity: a differenza di Jacobi, questo metodo non può essere parallelizzato, ma è necessario operare in modo sequenziale.

\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Quando converge?}}
\end{flushleft}
In tre casi:
\begin{enumerate}
    \item Se la matrice $\mathrm{A} \in \mathbb{R}^{n \times n}$ è a dominanza diagonale stressa per righe (vedi pag. \pageref{dominanza diagonale stressa per righe}), allora il metodo di Gauss-Seidel converge.

    \item Se la matrice $\mathrm{A}$ è simmetrica (uguale alla sua trasposta) e definita positiva (vedi pag. \pageref{matrice definita positiva}), allora Gauss-Seidel converge.

    \item Con la definizione di convergenza data per il metodo di Jacobi a pagina \pageref{definizione: convergenza di Jacobi e Gauss-Seidel}
\end{enumerate}
Da notare: \textbf{non esistono risultati generali che consentono di affermare che il metodo di Gauss-Seidel converga sempre più rapidamente di quello di Jacobi}, a parte il caso della definizione a pagina \pageref{matrice definita positiva}.

\newpage

\subsubsection{Il metodo di Richardson}

Una tecnica generale per costruire un metodo iterativo è basata sulla seguente \definition{decomposizione additiva} (o \definition{splitting}) della matrice $A$:
\begin{equation*}
    A = P - \left(P - A\right)
\end{equation*}
In cui $P$ è un'opportuna matrice non singolare ($\det\left(P\right) \ne 0$) chiamata \definition{precondizionatore} di $A$. Di conseguenza:
\begin{equation*}
    P\mathbf{x} = \left(P-A\right)\mathbf{x} + \mathbf{b}
\end{equation*}
È un sistema purché si ponga:
\begin{equation*}
    \begin{array}{rcl}
        B &=& P^{-1} \left(P-A\right) = I - P^{-1}A \\
        \mathbf{g} &=& P^{-1}\mathbf{b}
    \end{array}
\end{equation*}
Questa identità suggerisce la definizione del seguente metodo iterativo:
\begin{equation*}
    P\left(\mathbf{x}^{\left(k+1\right)} - \mathbf{x}^{\left(k\right)}\right) = \mathbf{r}^{\left(k\right)} \hspace{2em} k \ge 0
\end{equation*}
In cui:
\begin{equation}
    \mathbf{r}^{\left(k\right)} = \mathbf{b} - A\mathbf{x}^{\left(k\right)}
\end{equation}
Denota il vettore residuo alla $k$-esima iterazione. Una generalizzazione di questo metodo iterativo è
\begin{equation}\label{eq: metodo di Richardson}
    P\left(\mathbf{x}^{\left(k+1\right)} - \mathbf{x}^{\left(k\right)}\right) = \alpha_{k}\mathbf{r}^{\left(k\right)} \hspace{2em} k \ge 0
\end{equation}
Dove $\alpha_{k} \ne 0$ è un parametro che può cambiare ad ogni iterazione e che, a priori, servirà a migliorare le proprietà di convergenza della successione $\left\{\mathbf{x}^{\left(k\right)}\right\}$.

\highspace
L'equazione \ref{eq: metodo di Richardson} è nota come \definition{metodo di Richardson} o \definition{metodo di Richardson dinamico}; se $\alpha_{k} = \alpha$ per ogni $k \ge 0$ esso si dice \definition{metodo di Richardson stazionario}.

\highspace
Questo metodo richiede ad ogni passo di trovare il cosiddetto \definition{residuo precondizionato $\mathbf{z}^{\left(k\right)}$} dato dalla soluzione del sistema lineare:
\begin{equation}\label{eq: residuo precondizionato}
    P\mathbf{z}^{\left(k\right)} = \mathbf{r}^{\left(k\right)}
\end{equation}
Quindi, la nuova iterata è definita da $\mathbf{x}^{\left(k+1\right)} = \mathbf{x}^{\left(k\right)} + \alpha_{k}\mathbf{z}^{\left(k\right)}$.

Per questa ragione la matrice $P$ deve essere scelta in modo tale che il costo computazionale richiesto dalla risoluzione di \ref{eq: residuo precondizionato} sia modesto (ogni matrice $P$ diagonale, tridiagonale o triangolare andrebbe bene a questo scopo).

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Quando converge?}}
\end{flushleft}
Per studiare la convergenza, si definisce la \definition{norma dell'energia} associata alla matrice $A$:
\begin{equation}
    \left|\left| \mathbf{v} \right|\right|_{A} = \sqrt{\mathbf{v}^{T} A \mathbf{v}} \hspace{2em} \forall\mathbf{v} \in \mathbb{R}^{n}
\end{equation}
E si definisce la seguente definizione.

\begin{definitionbox}
    Sia $A \in \mathbb{R}^{n \times n}$.

    Per ogni matrice non singolare ($\det \ne 0$) $P \in \mathbb{R}^{n \times n}$ il \definition{metodo di Richardson stazionario} \underline{\textbf{converge}} se e solo se:
    \begin{equation*}
        \left| \lambda_{i} \right|^{2} < \dfrac{2}{\alpha} \mathrm{Re}\lambda_{i} \hspace{2em} \forall i = 1, \dots, n
    \end{equation*}
    In cui $\lambda_{i}$ sono gli autovalori della matrice risultato di $P^{-1} A$.

    \highspace
    In particolare, se gli autovalori della matrice risultato di $P^{-1} A$ sono reali, allora esso converge se e solo se:
    \begin{equation*}
        0 < \alpha \lambda_{i} < 2 \hspace{2em} \forall i = 1, \dots, n
    \end{equation*}

    \highspace
    Se $A$ e $P$ sono entrambe simmetriche (quindi uguali alle loro rispettive trasposte) e definite positive (definizione pagina \pageref{matrice definita positiva}) il metodo di Richardson stazionario \textbf{converge per ogni possibile scelta di} $\mathbf{x}^{\left(0\right)}$ se e solo se:
    \begin{equation*}
        0 < \alpha < \dfrac{2}{\lambda_{\max}}
    \end{equation*}
    Dove $\lambda_{\max}$ (che è maggiore di zero) è l'autovalore massimo della matrice risultato di $P^{-1} A$.

    \highspace
    E ancora, il raggio spettrale $\rho\left(B_{\alpha}\right)$ della matrice di iterazione $B_{\alpha} = I - \alpha P^{-1}A$ è il minimo quando $\alpha = \alpha_{\text{opt}}$, dove:
    \begin{equation}
        \alpha_{\text{opt}} = \dfrac{2}{\lambda_{\min} + \lambda_{\max}}
    \end{equation}
    Dove ovviamente $\lambda_{\min}$ è l'autovalore minimo della matrice risultato di $P^{-1} A$.

    \highspace
    Infine, se $\alpha = \alpha_{\text{opt}}$, vale la seguente \textbf{stima di convergenza}:
    \begin{equation}
        \left|\left| \mathbf{e}^{\left(k\right)} \right|\right|_{A} \le \left(
            \dfrac{K \left(P^{-1}A\right)-1}{K \left(P^{-1}A\right)+1}
        \right)^{k}
        \left|\left| \mathbf{e}^{\left(0\right)} \right|\right|_{A}
        \hspace{2em} k \ge 0
    \end{equation}
    Si noti che la massima velocità di convergenza è data anche dal raggio spettrale:
    \begin{equation}
        \rho_{\text{opt}} = \dfrac{K \left(P^{-1}A\right)-1}{K \left(P^{-1}A\right)+1}
    \end{equation}
\end{definitionbox}


