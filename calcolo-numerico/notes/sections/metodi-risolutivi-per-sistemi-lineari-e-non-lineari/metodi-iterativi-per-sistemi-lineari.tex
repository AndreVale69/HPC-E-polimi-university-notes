\subsection{Metodi iterativi per sistemi lineari}

Un \definition{metodo iterativo} per la risoluzione del sistema lineare $Ax = b$ con:
\begin{itemize}
    \item $A \in \mathbb{R}^{n \times n}$
    \item $b \in \mathbb{R}^{n}$
    \item $x \in \mathbb{R}^{n}$
    \item $\det\left(A\right) \ne 0$
\end{itemize}
Consiste nel costruire una successione di vettori del tipo:
\begin{equation*}
    \left\{\mathbf{x}^{\left(k\right)}, \: k \ge 0\right\}
\end{equation*}
Di $\mathbb{R}^{n}$ che \textbf{converge} alla soluzione esatta $\mathbf{x}$, ossia tale che:
\begin{equation}
    \displaystyle\lim_{k \rightarrow \infty} \mathbf{x}^{\left(k\right)} = \mathbf{x}
\end{equation}
Per un qualunque vettore iniziale $\mathbf{x}^{\left(0\right)} \in \mathbb{R}$, ossia la \textbf{convergenza non deve dipendere dalla scelta di} $\mathbf{x}^{\left(0\right)}$.

\highspace
Inoltre, si definisce ricorsivamente:
\begin{equation}\label{eq: metodi iterativi x con k + 1}
    \mathbf{x}^{\left(k+1\right)} = \mathrm{B}\mathbf{x}^{\left(k\right)} + \mathbf{g} \hspace{2em} k \ge 0
\end{equation}
Essendo:
\begin{itemize}
    \item $\mathrm{B}$ una matrice opportuna (dipendente da $\mathrm{A}$)
    \item $\mathbf{g}$ un vettore opportuno (dipendente da $\mathrm{A}$ e da $\mathbf{b}$)
\end{itemize}

\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{E come si possono scegliere questi valori?}}
\end{flushleft}
La scelta della matrice $\mathrm{B}$ e del vettore $\mathbf{g}$ è piuttosto semplice. Devono essere \textbf{rispettate due proprietà}:
\begin{itemize}
    \item \textbf{Condizione di consistenza}. I valori devono garantire che:
    \begin{equation}\label{eq: condizione di consistenza}
        \mathbf{x} = \mathrm{B}\mathbf{x} + \mathbf{g}
    \end{equation}
    Essendo $\mathbf{x} = \mathrm{A}^{-1}\mathbf{b}$, necessariamente dovrà aversi $\mathbf{g} = \left(\mathrm{I} - \mathrm{B}\right) \mathrm{A}^{-1} \mathbf{b}$. 

    \item \textbf{Condizione di convergenza}. Prima di dare la condizione, è necessario comprendere alcune cose. Prima di tutto, la seguente espressione:
    \begin{equation*}
        \mathbf{e}^{\left(k\right)} = \mathbf{x} - \mathbf{x}^{\left(k\right)}
    \end{equation*}
    Viene identificata come l'\textbf{errore al passo} $k$. Adesso sottraendo l'equazione \ref{eq: metodi iterativi x con k + 1} dalla condizione di consistenza \ref{eq: condizione di consistenza} si ottiene:
    \begin{equation*}
        \mathbf{e}^{\left(k+1\right)} = \mathrm{B}\mathbf{e}^{\left(k\right)}
    \end{equation*}
    Per tale ragione $\mathrm{B}$ viene detta \definition{matrice di iterazione} del metodo \ref{eq: metodi iterativi x con k + 1}.

    E adesso, si presenta la condizione di convergenza:
    \begin{itemize}
        \item La matrice $\mathrm{B}$ è simmetrica\footnote{Quindi corrisponde con la sua trasposta} e definita positiva\footnote{Definizione a pagina: \pageref{matrice definita positiva}}, allora:
        \begin{equation}
            \left|\left| \mathbf{e}^{\left(k+1\right)} \right|\right| 
            = 
            \left|\left| \mathrm{B}\mathbf{e}^{\left(k\right)} \right|\right|
            \le
            \rho\left(\mathrm{B}\right)
            \left|\left| \mathbf{e}^{\left(k\right)} \right|\right|
            \hspace{2em}
            \forall k \ge 0
        \end{equation}
        Dove $\rho\left(\mathrm{B}\right)$ è il \definition{raggio spettrale} di $\mathrm{B}$ ed è il massimo degli autovalori di $\mathrm{B}$. Si tenga conto che nel caso di matrici simmetriche e definite positive esso coincide con il massimo autovalore.

        \item Iterando a ritroso il punto precedente, si ottiene:
        \begin{equation}
            \left|\left| \mathbf{e}^{\left(k\right)} \right|\right| 
            \le
            \left[\rho\left(\mathrm{B}\right)\right]^{k}
            \left|\left| \mathbf{e}^{\left(0\right)} \right|\right| 
            \hspace{2em}
            k \ge 0
        \end{equation}
        Si noti che se $\rho\left(\mathrm{B}\right) < 1$, allora $\mathbf{e}^{\left(k\right)} \rightarrow \mathbf{0}$ per $k \rightarrow \infty$ per ogni possibile $\mathbf{e}^{0}$ (e, conseguentemente, per ogni $\mathbf{x}^{\left(0\right)}$).
    \end{itemize}
\end{itemize}
In generale, la condizione sufficiente e necessaria di convergenza è la seguente.

\begin{definitionbox}[: condizione sufficiente e necessaria di convergenza]
    Un metodo iterativo della forma dell'equazione \ref{eq: metodi iterativi x con k + 1}, la cui matrice di iterazione $B$ soddisfi la condizione di consistenza (eq. \ref{eq: condizione di consistenza}), è \textbf{convergente} per ogni $\mathbf{x}^{\left(0\right)}$ \emph{se e soltanto se} $\rho\left(\mathrm{B}\right) < 1$ (raggio spettrale minore di 1).

    \highspace
    Inoltre, minore è $\rho\left(\mathrm{B}\right)$, minore è il numero di iterazioni necessarie per ridurre l'errore iniziale di un dato fattore.
\end{definitionbox}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Perché introdurre i metodi iterativi?}}
\end{flushleft}
L'esigenza di introdurre i metodi iterativi sorge nel momento in cui si ragiona sulla quantità di tempo spesa da un calcolatore per eseguire la fattorizzazione LU su matrici di grandi dimensioni. Difatti, con matrici con ordini di $10^{7}$, sono necessari circa 11 giorni.

\newpage

\subsubsection{Il metodo di Jacobi}

Se i coefficienti diagonali della matrice $A$ non sono nulli, allora:
\begin{equation*}
    D = \mathrm{diag}\left(a_{11}, a_{22}, \dots, a_{nn}\right)
\end{equation*}
Ovvero $D$ è la matrice diagonale costruita a partire dagli elementi diagonali di $A$. 

\highspace
Il \definition{metodo di Jacobi} corrisponde alla forma:
\begin{equation*}
    \mathrm{D}\mathbf{x}^{\left(k+1\right)} = \mathbf{b} - \left(\mathrm{A} - \mathrm{D}\right)\mathbf{x}^{\left(k\right)} \hspace{2em} k \ge 0
\end{equation*}
Che per componenti assume la forma:
\begin{equation}
    x_{i}^{\left(k+1\right)}
    =
    \dfrac{1}{a_{ii}}
    \left(
        b_{i} - \displaystyle\sum_{j=1, j \ne i}^{n} a_{ij} x_{j}^{\left(k\right)}
    \right)
    \hspace{2em}
    i = 1, \dots, n
\end{equation}
Dove $k \ge 0$ e $\mathbf{x}^{\left(0\right)} = \left(x_{1}^{\left(0\right)}, x_{2}^{\left(0\right)}, \dots, x_{n}^{\left(0\right)}\right)^{T}$ è il vettore iniziale.

\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Quando converge?}}
\end{flushleft}
In due casi:
\begin{enumerate}
    \item Se la matrice $\mathrm{A} \in \mathbb{R}^{n \times n}$ è a dominanza diagonale stressa per righe\footnote{\label{dominanza diagonale stressa per righe}
        In algebra lineare una \definition{matrice a dominanza diagonale stretta per righe} o \definition{matrice diagonale dominante stretta per righe} è una matrice quadrata $A \in \mathbb{C}^{n \times n}$ di ordine $n$ i cui elementi diagonali sono maggiori (non stretta sarebbe maggiori o uguali) in valore assoluto della somma di tutti i restanti elementi della stessa riga in valore assoluto:
        \begin{equation}
            \left| a_{ii} \right| > \displaystyle\sum_{j=1, j \ne i}^{n} \left| a_{ij} \right| 
        \end{equation}
        In senso \underline{non stretto}:
        \begin{equation}
            \left| a_{ii} \right| \ge \displaystyle\sum_{j=1, j \ne i}^{n} \left| a_{ij} \right|
        \end{equation}
    }, allora il metodo di Jacobi converge.

    \item Con la seguente definizione (si veda il paragrafo \ref{subsubsection: il metodo di Gauss-Seidel} per comprendere meglio le definizioni):
    \begin{definitionbox}[: convergenza di Jacobi e Gauss-Seidel]\label{definizione: convergenza di Jacobi e Gauss-Seidel}
        Se $A \in \mathbb{R}^{n \times n}$ è una \definition{matrice tridiagonale}, quindi una matrice quadrata che al di fuori della diagonale principale e delle linee immediatamente al di sopra e al di sotto di essa (la prima sovradiagonale e la prima sottodiagonale), ha solo valori nulli. Un esempio di amtrice tridiagonale:
        \begin{equation*}
            \begin{bmatrix}
                1 & 4 & 0 & 0 \\
                3 & 4 & 1 & 0 \\
                0 & 2 & 3 & 4 \\
                0 & 0 & 1 & 3
            \end{bmatrix}
        \end{equation*}
        Se tale matrice è non singolare, quindi $\det\left(A\right) \ne 0$, con valori della diagonale diversi da zero, allora i metodi di Jacobi e di Gauss-Seidel sono \textbf{entrambi convergenti o entrambi divergenti}.
        \begin{itemize}
            \item Se convergono, la velocità dei metodi:
            \begin{equation*}
                \text{Gauss-Seidel} \gg \text{Jacobi}
            \end{equation*}
            Precisamente il raggio spettrale della matrice di iterazione del metodo di Gauss-Seidel è il quadrato del raggio spettrale di quella del metodo di Jacobi.
        \end{itemize}
    \end{definitionbox}
\end{enumerate}

\highspace
HPC curiosity: è interessante notare che il metodo di Jacobi viene facilmente parallelizzato per aumentare le prestazioni di calcolo.

\newpage

\subsubsection{Il metodo di Gauss-Seidel}\label{subsubsection: il metodo di Gauss-Seidel}

Con il metodo di Jacobi, ogni componente $x_{i}^{\left(k+1\right)}$ del nuovo vettore $\mathbf{x}^{\left(k+1\right)}$ viene calcolata indipendentemente dalle altre. Questa può essere la base di partenza per costruire un nuovo metodo più ottimizzato, poiché se per il calcolo di $x_{i}^{\left(k+1\right)}$ venissero usate le nuove componenti già disponibili $x_{j}^{\left(k+1\right)}$, $j = 1, \dots, i-1$, assieme con le vecchie $x_{j}^{\left(k\right)}$, $j \ge i$.

\highspace
Supponendo che gli elementi della diagonale non siano nulli, per $k \ge 0$, il \definition{metodo di Gauss-Seidel}:
\begin{equation}\label{eq: metodo di Gauss-Seidel}
    x_{i}^{\left(k+1\right)} = 
    \dfrac{1}{a_{ii}} \left(
        b_{i} -
        \displaystyle\sum_{j=1}^{i-1} a_{ij}x_{j}^{\left(k+1\right)} -
        \displaystyle\sum_{j=i+1}^{n} a_{ij}x_{j}^{\left(k\right)}
    \right)
    \hspace{2em}
    i = 1, \dots, n
\end{equation}
HPC curiosity: a differenza di Jacobi, questo metodo non può essere parallelizzato, ma è necessario operare in modo sequenziale.

\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Quando converge?}}
\end{flushleft}
In tre casi:
\begin{enumerate}
    \item Se la matrice $\mathrm{A} \in \mathbb{R}^{n \times n}$ è a dominanza diagonale stressa per righe (vedi pag. \pageref{dominanza diagonale stressa per righe}), allora il metodo di Gauss-Seidel converge.

    \item Se la matrice $\mathrm{A}$ è simmetrica (uguale alla sua trasposta) e definita positiva (vedi pag. \pageref{matrice definita positiva}), allora Gauss-Seidel converge.

    \item Con la definizione di convergenza data per il metodo di Jacobi a pagina \pageref{definizione: convergenza di Jacobi e Gauss-Seidel}
\end{enumerate}
Da notare: \textbf{non esistono risultati generali che consentono di affermare che il metodo di Gauss-Seidel converga sempre più rapidamente di quello di Jacobi}, a parte il caso della definizione a pagina \pageref{matrice definita positiva}.

\newpage

\subsubsection{Il metodo di Richardson}

Una tecnica generale per costruire un metodo iterativo è basata sulla seguente \definition{decomposizione additiva} (o \definition{splitting}) della matrice $A$:
\begin{equation*}
    A = P - \left(P - A\right)
\end{equation*}
In cui $P$ è un'opportuna matrice non singolare ($\det\left(P\right) \ne 0$) chiamata \definition{precondizionatore} di $A$. Di conseguenza:
\begin{equation*}
    P\mathbf{x} = \left(P-A\right)\mathbf{x} + \mathbf{b}
\end{equation*}
È un sistema purché si ponga:
\begin{equation*}
    \begin{array}{rcl}
        B &=& P^{-1} \left(P-A\right) = I - P^{-1}A \\
        \mathbf{g} &=& P^{-1}\mathbf{b}
    \end{array}
\end{equation*}
Questa identità suggerisce la definizione del seguente metodo iterativo:
\begin{equation*}
    P\left(\mathbf{x}^{\left(k+1\right)} - \mathbf{x}^{\left(k\right)}\right) = \mathbf{r}^{\left(k\right)} \hspace{2em} k \ge 0
\end{equation*}
In cui:
\begin{equation}
    \mathbf{r}^{\left(k\right)} = \mathbf{b} - A\mathbf{x}^{\left(k\right)}
\end{equation}
Denota il vettore residuo alla $k$-esima iterazione. Una generalizzazione di questo metodo iterativo è
\begin{equation}\label{eq: metodo di Richardson}
    P\left(\mathbf{x}^{\left(k+1\right)} - \mathbf{x}^{\left(k\right)}\right) = \alpha_{k}\mathbf{r}^{\left(k\right)} \hspace{2em} k \ge 0
\end{equation}
Dove $\alpha_{k} \ne 0$ è un parametro che può cambiare ad ogni iterazione e che, a priori, servirà a migliorare le proprietà di convergenza della successione $\left\{\mathbf{x}^{\left(k\right)}\right\}$.

\highspace
L'equazione \ref{eq: metodo di Richardson} è nota come \definition{metodo di Richardson} o \definition{metodo di Richardson dinamico}; se $\alpha_{k} = \alpha$ per ogni $k \ge 0$ esso si dice \definition{metodo di Richardson stazionario}.

\highspace
Questo metodo richiede ad ogni passo di trovare il cosiddetto \definition{residuo precondizionato $\mathbf{z}^{\left(k\right)}$} dato dalla soluzione del sistema lineare:
\begin{equation}\label{eq: residuo precondizionato}
    P\mathbf{z}^{\left(k\right)} = \mathbf{r}^{\left(k\right)}
\end{equation}
Quindi, la nuova iterata è definita da $\mathbf{x}^{\left(k+1\right)} = \mathbf{x}^{\left(k\right)} + \alpha_{k}\mathbf{z}^{\left(k\right)}$.

Per questa ragione la matrice $P$ deve essere scelta in modo tale che il costo computazionale richiesto dalla risoluzione di \ref{eq: residuo precondizionato} sia modesto (ogni matrice $P$ diagonale, tridiagonale o triangolare andrebbe bene a questo scopo).

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Quando converge?}}
\end{flushleft}
Per studiare la convergenza, si definisce la \definition{norma dell'energia} associata alla matrice $A$:
\begin{equation}\label{eq: norma dell'energia}
    \left|\left| \mathbf{v} \right|\right|_{A} = \sqrt{\mathbf{v}^{T} A \mathbf{v}} \hspace{2em} \forall\mathbf{v} \in \mathbb{R}^{n}
\end{equation}
E si definisce la seguente definizione.

\begin{definitionbox}
    Sia $A \in \mathbb{R}^{n \times n}$.

    Per ogni matrice non singolare ($\det \ne 0$) $P \in \mathbb{R}^{n \times n}$ il \definition{metodo di Richardson stazionario} \underline{\textbf{converge}} se e solo se:
    \begin{equation*}
        \left| \lambda_{i} \right|^{2} < \dfrac{2}{\alpha} \mathrm{Re}\lambda_{i} \hspace{2em} \forall i = 1, \dots, n
    \end{equation*}
    In cui $\lambda_{i}$ sono gli autovalori della matrice risultato di $P^{-1} A$.

    \highspace
    In particolare, se gli autovalori della matrice risultato di $P^{-1} A$ sono reali, allora esso converge se e solo se:
    \begin{equation*}
        0 < \alpha \lambda_{i} < 2 \hspace{2em} \forall i = 1, \dots, n
    \end{equation*}

    \highspace
    Se $A$ e $P$ sono entrambe simmetriche (quindi uguali alle loro rispettive trasposte) e definite positive (definizione pagina \pageref{matrice definita positiva}) il metodo di Richardson stazionario \textbf{converge per ogni possibile scelta di} $\mathbf{x}^{\left(0\right)}$ se e solo se:
    \begin{equation*}
        0 < \alpha < \dfrac{2}{\lambda_{\max}}
    \end{equation*}
    Dove $\lambda_{\max}$ (che è maggiore di zero) è l'autovalore massimo della matrice risultato di $P^{-1} A$.

    \highspace
    E ancora, il raggio spettrale $\rho\left(B_{\alpha}\right)$ della matrice di iterazione $B_{\alpha} = I - \alpha P^{-1}A$ è il minimo quando $\alpha = \alpha_{\text{opt}}$, dove:
    \begin{equation}
        \alpha_{\text{opt}} = \dfrac{2}{\lambda_{\min} + \lambda_{\max}}
    \end{equation}
    Dove ovviamente $\lambda_{\min}$ è l'autovalore minimo della matrice risultato di $P^{-1} A$.

    \highspace
    Infine, se $\alpha = \alpha_{\text{opt}}$, vale la seguente \textbf{stima di convergenza}:
    \begin{equation}
        \left|\left| \mathbf{e}^{\left(k\right)} \right|\right|_{A} \le \left(
            \dfrac{K \left(P^{-1}A\right)-1}{K \left(P^{-1}A\right)+1}
        \right)^{k}
        \left|\left| \mathbf{e}^{\left(0\right)} \right|\right|_{A}
        \hspace{2em} k \ge 0
    \end{equation}
    Si noti che la massima velocità di convergenza è data anche dal raggio spettrale:
    \begin{equation}
        \rho_{\text{opt}} = \dfrac{K \left(P^{-1}A\right)-1}{K \left(P^{-1}A\right)+1}
    \end{equation}
\end{definitionbox}

\newpage

\subsubsection{Il metodo del Gradiente e del Gradiente Coniugato}

Si definisce la funzione $\varPhi: \mathbb{R}^{n} \rightarrow \mathbb{R}$:
\begin{equation}
    \varPhi\left(\mathbf{x}\right) = \dfrac{1}{2}\mathbf{x}^{T} A \mathbf{x} - \mathbf{x}^{T}\mathbf{b}
\end{equation}
Nel caso in cui la matrice $A$ è simmetrica\footnote{Quindi corrisponde con la sua trasposta} e definita positiva\footnote{Definizione a pagina: \pageref{matrice definita positiva}}, $\varPhi$ è una funzione convessa, cioè tale che ogni per ogni $\mathbf{x}, \mathbf{y} \in \mathbb{R}^{n}$ e per ogni $\alpha \in \left[0,1\right]$ vale:
\begin{equation}
    \varPhi\left(
        \alpha\mathbf{x} + \left(1-\alpha\right) \mathbf{y}
    \right)
    \le
    \alpha\varPhi\left(\mathbf{x}\right) + \left(1-\alpha\right)\varPhi\left(\mathbf{y}\right)
\end{equation}
Ed ammette un unico punto stazionario $\mathbf{x}^{*}$ che è anche un \textbf{punto di minimo locale ed assoluto}. Quindi:
\begin{equation}\label{eq: problema di minimo - metodo del gradiente}
    \mathbf{x}^{*} = \underset{\mathbf{x} \in \mathbb{R}^{n}}{\mathrm{argmin}} \: \varPhi\left(\mathbf{x}\right)
\end{equation}
È l'unica soluzione dell'equazione:
\begin{equation}\label{eq: direzione massima crescita}
    \nabla\varPhi\left(\mathbf{x}\right) = A\mathbf{x} - \mathbf{b} = \mathbf{0}
\end{equation}
\textbf{Risolvere} il \textbf{problema di minimo} dell'equazione \ref{eq: problema di minimo - metodo del gradiente} \textbf{equivale} pertanto a \textbf{risolvere il sistema lineare}\footnote{Il gradiente è un argomento del calcolo differenziale vettoriale e il vettore gradiente di una funzione scalare punta secondo la direzione di massima crescita della funzione stessa. Se si è un po' arrugginiti a riguardo, si può dare un'occhiata su \href{https://it.wikipedia.org/wiki/Gradiente}{Wikipedia} o su \href{https://www.youmath.it/lezioni/analisi-due/varie/2275-gradiente.html}{YouMath}}.

\highspace
Per un generico $\overline{\mathbf{x}} \in \mathbb{R}^{n}$ diverso da $\mathbf{x}^{*}$, $\nabla\varPhi\left(\mathbf{x}\right)$ è un vettore non nullo di $\mathbb{R}^{n}$ che individua la direzione lungo cui avviene:
\begin{itemize}
    \item La \textbf{massima \underline{crescita}} di $\varPhi$;
    \item Di conseguenza $-\nabla\varPhi\left(\overline{\mathbf{x}}\right)$ individua la direzione di \textbf{massima \underline{decrescita}} di $\varPhi$ a partire da $\overline{\mathbf{x}}$.
\end{itemize}
Inoltre, ricordando che il vettore residuo nel punto $\overline{\mathbf{x}}$ è $\overline{\mathbf{r}} = \mathbf{b} - A\overline{\mathbf{x}}$, grazie alla direzione lungo cui avviene la massima crescita \ref{eq: direzione massima crescita}, si ha che il \textbf{vettore residuo} è uguale a:
\begin{equation}
    \overline{\mathbf{r}} = -\nabla\varPhi\left(\overline{\mathbf{x}}\right)
\end{equation}
Cioè il residuo \textbf{individua una possibile direzione lungo cui muoversi per avvicinarsi al punto di minimo} $\mathbf{x}^{*}$.

\highspace
In generale, un vettore $\mathbf{d}$ rappresenta una \definition{direzione di discesa per $\varPhi$} nel punto $\overline{\mathbf{x}}$ se si verificano le seguenti condizioni:
\begin{equation}
    \begin{array}{rcl}
        \nabla\varPhi\left(\overline{\mathbf{x}}\right) \ne \mathbf{0} &\Rightarrow& \mathbf{d}^{T}\nabla\varPhi\left(\overline{\mathbf{x}}\right) < 0 \\ [.5em]
        \nabla\varPhi\left(\overline{\mathbf{x}}\right) = \mathbf{0} &\Rightarrow& \mathbf{d} = \mathbf{0}
    \end{array}
\end{equation}
Il \textbf{residuo} è pertanto una \textbf{direzione di discesa}. È banale dire che quindi per \textbf{avvicinarsi al punto di minimo} ci si \textbf{muoverà lungo opportune direzioni di discesa}. I \definition{metodi di discesa} sono definiti nel seguente modo.

\begin{definitionbox}[: metodi di discesa]
    Assegnato un vettore $\mathbf{x}^{\left(0\right)} \in \mathbb{R}^{n}$, per $k = 0, 1, \dots$ fino a convergenza:
    \begin{enumerate}
        \item Si determina una direzione di discesa $\mathbf{d}^{\left(k\right)} \in \mathbb{R}^{n}$
        \item Si determina un passo $\alpha_{k} \in \mathbb{R}$
        \item Si pone $\mathbf{x}^{\left(k+1\right)} = \mathbf{x}^{\left(k\right)} + \alpha_{k}\mathbf{d}^{\left(k\right)}$
    \end{enumerate}
\end{definitionbox}

\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Ma quindi quale è la differenza tra Gradiente e Gradiente Coniugato?}}
\end{flushleft}
I metodi del gradiente e del gradiente coniugato sono metodi di discesa che si \textbf{differenziano nella scelta delle direzioni di discesa}, mentre la scelta dei passi $\alpha_{k}$ è \emph{comune} ad entrambi. Per cui, qua di seguito si esami la scelta dei passi, supponendo di aver già calcolato la nuova direzione $\mathbf{d}^{\left(k\right)}$.

\highspace
La \textbf{scelta del passo} $\alpha_{k}$ è comune ad entrambi, quindi:
\begin{equation}
    \alpha_{k} = \underset{\alpha \in \mathbb{R}^{n}}{\mathrm{argmin}} \: \varPhi\left(\mathbf{x}^{\left(k\right)} + \alpha\mathbf{d}^{\left(k\right)}\right)
\end{equation}
Ovvero un passo $\alpha_{k}$ tale che $\varPhi\left(\mathbf{x}^{\left(k+1\right)}\right)$ sia il \textbf{minimo} di $\varPhi$ lungo la direzione $\mathbf{d}^{\left(k\right)}$ passante per $\mathbf{x}^{\left(k\right)}$.

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{bookmark} \textbf{Definizione metodo del gradiente}}
\end{flushleft}
Adesso si può dare la definizione del Gradiente. Il \definition{metodo del gradiente} è caratterizzato dalla scelta:
\begin{equation}
    \mathbf{d}^{\left(k\right)} = \mathbf{r}^{\left(k\right)} = -\nabla\varPhi\left(\mathbf{x}^{\left(k\right)}\right) \hspace{2em} k = 0, 1, \dots
\end{equation}
Quindi la \textbf{direzione di discesa ad ogni passo $k$ è la direzione opposta a quella del gradiente} (come si vede dal meno davanti a $\nabla$) della funzione $\varPhi$.

\highspace
L'\definition{algoritmo del metodo del gradiente} è: dato $\mathbf{x}^{\left(0\right)} \in \mathbb{R}^{n}$, si definisce $\mathbf{r}^{\left(0\right)} = \mathbf{b} - A\mathbf{x}^{\left(0\right)}$ e si calcola:
\begin{equation}
    \begin{array}{rl}
        \text{per } k = 0, 1, \dots & \\
        & \alpha_{k} = \dfrac{
            \left(\mathbf{r}^{\left(k\right)}\right)^{T} \mathbf{r}^{\left(k\right)}
        }{
            \left(\mathbf{r}^{\left(k\right)}\right)^{T} A \mathbf{r}^{\left(k\right)}
        } \\ [2em]
        & \mathbf{x}^{\left(k+1\right)} = \mathbf{x}^{\left(k\right)} + \alpha_{k}\mathbf{r}^{\left(k\right)} \\ [1em]
        & \mathbf{r}^{\left(k+1\right)} = \mathbf{r}^{\left(k\right)} - \alpha_{k} A \mathbf{r}^{\left(k\right)}
    \end{array}
\end{equation}

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Quando converge il metodo del gradiente?}}
\end{flushleft}
Se $A \in \mathbb{R}^{n \times n}$ è simmetrica\footnote{Quindi corrisponde con la sua trasposta} e definita positiva\footnote{Definizione a pagina: \pageref{matrice definita positiva}} il metodo del gradiente converge alla soluzione del sistema lineare $A\mathbf{x} = \mathbf{b}$ per ogni dato iniziale $\mathbf{x}^{\left(0\right)} \in \mathbb{R}^{n}$ e inoltre vale la seguente \textbf{stima dell'errore}:
\begin{equation}
    \left|\left| \mathbf{e}^{\left(k\right)} \right|\right|_{A} \le \left(\dfrac{K\left(A\right)-1}{K\left(A\right)+1}\right)^{k} \left|\left| \mathbf{e}^{\left(0\right)} \right|\right|_{A} \hspace{2em} k \ge 0
\end{equation}
Dove $\left|\left| \cdot \right|\right|$ è la \definition{norma dell'energia} definita a pagina \pageref{eq: norma dell'energia} e $K\left(A\right)$ è il numero di condizionamento (spettrale) della matrice $A$ definito a pagina \pageref{eq: numero di condizionamento (spettrale)}.

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{Ma si può fare di meglio! Ed ecco che arriva il metodo del gradiente coniugato...}}
\end{flushleft}
È possibile costruire delle direzioni di discesa \dquotes{migliori} al fine di \textbf{giungere a convergenza in un numero di iterazioni inferiore rispetto a quelle del metodo del gradiente}. Partendo dunque dal metodo del gradiente e sfruttando la definizione ricorsiva dei vettori $\mathbf{x}^{\left(k\right)}$, si può ottenere:
\begin{equation}\label{eq: metodo del gradiente migliorato}
    \begin{array}{rcl}
        \mathbf{x}^{\left(k+1\right)} &=& \mathbf{x}^{\left(k\right)} + \alpha_{k}\mathbf{d}^{\left(k\right)} \\ [.5em]
        &=& \mathbf{x}^{\left(k-1\right)} + \alpha_{k-1} \mathbf{d}^{\left(k-1\right)} + \alpha_{k}\mathbf{d}^{\left(k\right)} \\ [.5em]
        &=& \dots \\ [.5em]
        &=& \mathbf{x}^{\left(0\right)} + \displaystyle\sum_{j=0}^{k} \alpha_{j}\mathbf{d}^{\left(j\right)}
    \end{array}
\end{equation}
Si deduce che il vettore $\left(\mathbf{x}^{\left(k+1\right)} - \mathbf{x}^{\left(0\right)}\right) \in \mathbb{R}^{n}$ è una combinazione lineare delle direzioni di discesa $\left\{\mathbf{d}^{\left(j\right)}\right\}_{j=0}^{k}$.

\highspace
Il \definition{metodo del gradiente coniugato} costruisce un sistema di direzioni di discesa in $\mathbb{R}^{n}$ che siano tutte linearmente indipendenti fra di loro (quindi $\left\{\mathbf{d}^{\left(k\right)}\right\}_{k=0}^{n-1}$ sarà una base di $\mathbb{R}^{n}$) e tali che i valori $\left\{\alpha_{k}\right\}_{k=0}^{n-1}$ siano proprio i coefficienti dello sviluppo di $\left(\mathbf{x}^{*} - \mathbf{x}^{\left(0\right)}\right)$ rispetto alla base $\left\{\mathbf{d}^{\left(k\right)}\right\}_{k=0}^{n-1}$. Questo comporta che l'$n$-simo termine $\mathbf{x}^{\left(n\right)}$ della successione (eq. \ref{eq: metodo del gradiente migliorato}) coincide con la soluzione esatta $\mathbf{x}^{*}$.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Quando converge il metodo del gradiente coniugato?}}
\end{flushleft}
Sia $A \in \mathbb{R}^{n \times n}$ una matrice simmetrica e definita positiva. Il metodo del gradiente coniugato per risolvere un sistema lineare \textbf{converge al più in $n$ iterazioni} (in aritmetica esatta). Inoltre, il residuo $\mathbf{r}^{\left(k\right)}$ alla $k$-esima iterazione (con $k < n$) è ortogonale a $\mathbf{d}^{\left(j\right)}$ per $j = 0, \dots, k-1$ e:
\begin{equation}
    \left|\left| \mathbf{e}^{\left(k\right)} \right|\right|_{A} \le \dfrac{2c^{k}}{1+c^{2k}} \: \left|\left| \mathbf{e}^{\left(0\right)} \right|\right|_{A}
\end{equation}
Con:
\begin{equation}
    c = \dfrac{
        \sqrt{K\left(A\right)}-1
    }{
        \sqrt{K\left(A\right)}+1
    }
\end{equation}
In aritmetica esatta il metodo del gradiente coniugato è pertanto un \textbf{metodo diretto in quanto termina dopo un numero finito di iterazioni}!

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{book} \textbf{Il precondizionamento migliora il metodo del gradiente}}
\end{flushleft}
Le iterazioni del metodo del gradiente, quando convergono alla soluzione esatta $\mathbf{x}^{*}$, sono caratterizzate da una \textbf{velocità di convergenza dipendente dal numero di condizionamento} della matrice $A$: \textbf{più grande è il numero di condizionamento di $A$ e tanto più lenta sarà la convergenza}. Quindi, invece di risolvere il sistema lineare, si può risolvere il cosiddetto \definition{sistema precondizionato} (equivalente):
\begin{equation}
    P_{L}^{-1}A P_{R}^{-1} \widehat{\mathbf{x}} = P_{L}^{-1}\mathbf{b} \hspace{2em} \text{con } \widehat{\mathbf{x}} = P_{R}\mathbf{x}
\end{equation}
Dove $P_{L}$ e $P_{R}$ sono opportune matrici non singolari tali che:
\begin{enumerate}
    \item La matrice:
    \begin{equation}
        P = P_{L}P_{R}
    \end{equation}
    è detta \definition{precondizionatore}, è simmetrica e definita positiva;

    \item $K\left(P^{-1}A\right) \ll K\left(A\right)$
    
    \item La risoluzione di sistemi lineari della forma:
    \begin{equation}
        P_{L}\mathbf{s} = \mathbf{t} \hspace{2em} P_{R}\mathbf{v} = \mathbf{w}
    \end{equation}
    è computazionalmente molto meno costosa della risoluzione del sistema originario $A\mathbf{x} = \mathbf{b}$.
\end{enumerate}
Se si sceglie $P_{R}$ uguale alla matrice identità, la matrice $P = P_{L}$ è detta \definition{precondizionatore sinistro}, ed il sistema precondizionato assume la forma:
\begin{equation}
    P^{-1}A\mathbf{x} = P^{-1}\mathbf{b}
\end{equation}
Analogamente, se $P_{L}$ è la matrice identità, la matrice $P = P_{R}$ è detta \definition{precondizionatore destro}, ed il sistema precondizionato assume la forma:
\begin{equation}
    AP^{-1}\widehat{\mathbf{x}} = \mathbf{b} \hspace{2em} \text{con }\widehat{\mathbf{x}} = P\mathbf{x}
\end{equation}
Il \definition{metodo del gradiente precondizionato} si ottiene applicando il metodo del gradiente al sistema precondizionato in cui si prenda $P_{R} = P_{L}^{T}$ e quindi $P_{R}^{-1} = P_{L}^{-T} \equiv \left(P_{L}^{-1}\right)^{T}$. Più precisamente, riscrivendo:
\begin{equation}
    \underbracket{P_{L}^{-1}AP_{L}^{-T}}_{\widehat{A}} \underbracket{P_{L}^{T}\mathbf{x}}_{\widehat{\mathbf{x}}} = \underbracket{P_{L}^{-1} \mathbf{b}}_{\widehat{\mathbf{b}}}
\end{equation}
