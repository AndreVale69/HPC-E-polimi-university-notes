\section{Metodi risolutivi per sistemi lineari e non lineari}

\subsection{Metodi diretti per sistemi lineari}

\subsubsection{Metodo delle sostituzioni in avanti e all'indietro}

\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Perché sono importanti i metodi numerici?}}
\end{flushleft}
Si consideri il seguente \textbf{sistema lineare}:
\begin{equation*}
    Ax = b
\end{equation*}
Dove:
\begin{itemize}
    \item $A \in \mathbb{R}^{n \times n}$ di componenti $a_{ij}$ e $b \in \mathbb{R}^{n}$ sono valori noti.

    \item $x \in \mathbb{R}^{n}$ è il vettore delle incognite.
    
    \item La costante $n$ rappresenta il numero di equazioni lineari delle incognite $x_{j}$.
\end{itemize}
Con queste caratteristiche, è possibile rappresentare la $i$-esima equazione nel seguente modo:
\begin{equation*}
    \displaystyle\sum_{j=1}^{n} a_{ij} x_{j} = b_{i} \:\: \rightarrow \:\: a_{1i}x_{1} + a_{i2} x_{2} + \cdots + a_{in}x_{n} = b_{i} \hspace{2em} \forall i = 1, \dots , n
\end{equation*}
La \textbf{soluzione esatta} del sistema, chiamata \definition{formula di Cramer}, è:
\begin{equation}\label{eq: formula di Cramer}
    x_{j} = \dfrac{\det\left(A_{j}\right)}{\det\left(A\right)}
\end{equation}
Con $A_{j} = \left|a_{1} \:\: \dots \:\: a_{j-1} \:\: b \:\: a_{j+1} \:\: \dots \:\: a_{n} \right|$ e $a_{i}$ le colonne di $A$. Ovviamente la soluzione \textbf{esiste ed è unica se il determinante} della matrice $A$ è \textbf{diverso da zero}:
\begin{equation*}
    \det\left(A\right) \ne 0
\end{equation*}
Purtroppo questo metodo è \textbf{inutilizzabile} poiché il calcolo di un determinante richiede all'incirca $n!$ (fattoriale di $n$) operazioni.

\highspace
Risulta evidente che sia necessario uno studio approfondito di \textbf{metodi numerici che si traducano in algoritmi efficienti} da farli eseguire su calcolatori. Nelle seguenti pagine si introducono i primi due algoritmi \dquotes{efficienti}.

\newpage

\begin{definitionbox}\label{metodo delle sostituzioni in avanti}
    Il seguente algoritmo rappresenta il \definition{metodo delle sostituzioni in avanti}. Dati:
    \begin{itemize}
        \item $L \in \mathbb{R}^{n \times n}$ matrice triangolare inferiore non singolare (cioè con determinante diverso da zero $\det\left(L\right) \ne 0$)
        
        \item $\mathbf{b} \in \mathbb{R}^{n}$ vettore termine noto
    \end{itemize}
    La soluzione è data da $Lx = \mathbf{b}$ con $x \in \mathbb{R}^{n}$. Più in generale si ha:
    \begin{equation}\label{eq: metodo delle sostituzioni in avanti}
        x_{i} = \dfrac{
            b_{i} - \displaystyle\sum_{j=1}^{i=1} L_{ij} \: x_{j}
        }{
            L_{ii}
        }
    \end{equation}
\end{definitionbox}

\noindent
Per comprendere meglio la definizione del metodo di sostituzioni in avanti, è possibile visualizzare in modo generale la matrice triangolare inferiore $L$ (non singolare):
\begin{equation*}
    L_{n \times n} = \begin{bmatrix}
        L_{11} & 0 & 0 & 0 & 0 \\
         & \ddots & 0 & 0 & 0 \\
         & & \ddots & 0 & 0 \\
         & L_{ij} & & \ddots & 0 \\
         & & & & L_{nn}
    \end{bmatrix}
\end{equation*}
Dove ovviamente $n$ è la grandezza della matrice quadrata. Dalla matrice, è possibile rappresentare le prime tre iterazioni, ovvero $x_{1}$, $x_{2}$ e $x_{3}$:
\begin{itemize}
    \item La prima riga è:
    \begin{equation*}
        x_{1} = \dfrac{
            b_{1}
        }{
            L_{11}
        }
    \end{equation*}
    Si può scrivere anche in linea nel seguente modo: $L_{11}x_{1} = b_{1}$.

    \item La seconda riga:
    \begin{equation*}
        x_{2} = \dfrac{
            b_{2} - (L_{21} \cdot x_{1} + L_{22}  \cdot \cancelto{0}{x_{2}})
        }{
            L_{22}
        }
    \end{equation*}
    In cui $x_{1}$ è il risultato del punto precedente e $x_{2}$ è il risultato che attualmente si sta calcolando, quindi uguale a zero.

    \item La terza riga:
    \begin{equation*}
        x_{3} = \dfrac{
            b_{3} - (L_{31} \cdot x_{1} + L_{32} \cdot x_{2} + L_{33} \cdot \cancelto{0}{x_{3}})
        }{
            L_{33}
        }
    \end{equation*}
\end{itemize}

\noindent
Il \textbf{numero di operazioni} richieste dal metodo delle sostituzioni in avanti è dato da 1 sottrazione, $i-1$ moltiplicazioni, $i-2$ addizioni e 1 divisione:
\begin{equation}
    \# op. = \displaystyle\sum_{i=1}^{n} \left(i-1\right) + \left(i-2\right) + 1 + 1 = \sum_{i=1}^{n} \left(2i-1\right) = n^{2}
\end{equation}
Per completezza si presenta anche il metodo delle sostituzioni all'indietro.

\begin{definitionbox}\label{metodo delle sostituzioni all'indietro}
    Il seguente algoritmo rappresenta il \definition{metodo delle sostituzioni all'indietro}. Dati:
    \begin{itemize}
        \item $U \in \mathbb{R}^{n \times n}$ matrice triangolare superiore non singolare (cioè con determinante diverso da zero $\det\left(U\right) \ne 0$)
        
        \item $\mathbf{b} \in \mathbb{R}^{n}$ vettore termine noto
    \end{itemize}
    La soluzione è data da $Ux = \mathbf{b}$ con $x \in \mathbb{R}^{n}$. Più in generale si ha:
    \begin{equation}\label{eq: metodo delle sostituzioni all'indietro}
        x_{i} = \dfrac{
            b_{i} - \displaystyle\sum_{j=i+1}^{n} U_{ij} \: x_{j}
        }{
            U_{ii}
        }
    \end{equation}
\end{definitionbox}

\noindent
Come per il metodo precedente, anche in questo caso è utile visualizzare la matrice generale:
\begin{equation*}
    U_{n \times n} = \begin{bmatrix}
        U_{11} &   &   &   &   \\
        0 & \ddots &   & U_{ij} & \\
        0 & 0 & \ddots &   &   \\
        0 & 0 & 0 & \ddots &   \\
        0 & 0 & 0 & 0 & U_{nn}
    \end{bmatrix}
\end{equation*}
Anche da questa matrice è possibile rappresentare le iterazioni:
\begin{itemize}
    \item La prima riga è:
    \begin{equation*}
        x_{1} = \dfrac{
            b_{1} - (U_{12} \cdot x_{2} + U_{13} \cdot x_{3} + U_{14} \cdot x_{4})
        }{
            U_{11}
        }
    \end{equation*}

    \item La seconda riga è:
    \begin{equation*}
        x_{2} = \dfrac{
            b_{2} - (U_{23} \cdot x_{3} + U_{24} \cdot x_{4})
        }{
            U_{22}
        }
    \end{equation*}

    \item La terza riga è:
    \begin{equation*}
        x_{3} = \dfrac{
            b_{3} - (U_{34} \cdot x_{4})
        }{
            U_{33}
        }
    \end{equation*}

    \item L'ultima riga è:
    \begin{equation*}
        x_{4} = \dfrac{
            b_{4}
        }{
            U_{44}
        }
    \end{equation*}
\end{itemize}
Si deduce ovviamente che l'ultima riga può essere generalizzata nel seguente modo:
\begin{equation*}
    x_{n} = \dfrac{b_{n}}{U_{nn}}
\end{equation*}

\noindent
Infine, il \textbf{numero di operazioni} è il medesimo del metodo delle sostituzioni in avanti, quindi $n^{2}$.

\newpage

\subsubsection{Fattorizzazione LRU: MEG e Cholesky}

Sia $A \in \mathbb{R}^{n \times n}$. Si supponga che esistano due opportune matrici $L$ ed $U$, triangolare inferiore e superiore, rispettivamente, tali che:
\begin{equation}\label{eq: fattorizzazione LU}
    A = LU
\end{equation}
L'equazione viene chiamata \definition{fattorizzazione LU} (o \textbf{decomposizione LU}) di $A$. 

\highspace
La fattorizzazione LU è stata introdotta poiché se $A$ \textbf{non è singolare} (quindi il determinante è diverso da zero) tali matrici devono essere \textbf{anch'esse non singolari}; questo assicura che i loro \textbf{elementi diagonali siano non nulli}. Da questa osservazione, si ottiene un risultato interessante perché la risoluzione di $Ax = b$ è \emph{equivalente} alla risoluzione dei due seguenti sistemi triangolari:
\begin{equation}
    Ly = b \hspace{2em} Ux = y
\end{equation}
Dove $y$ rappresenta la soluzione dell'equazione \ref{eq: metodo delle sostituzioni in avanti} a pagina \pageref{eq: metodo delle sostituzioni in avanti}, ovvero la risoluzione del metodo delle sostituzioni in avanti. Analogamente, la $x$ rappresenta la soluzione dell'equazione \ref{eq: metodo delle sostituzioni all'indietro} a pagina \pageref{eq: metodo delle sostituzioni all'indietro}, ovvero la risoluzione del metodo delle sostituzioni all'indietro.

\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Chiaro, ma che algoritmi esistono per calcolare la fattorizzazione LU?}}
\end{flushleft}
Esistono principalmente due algoritmi: il \definition{Metodo di Eliminazione di Gauss (MEG)} e la \definition{Fattorizzazione di Cholesky}.
\begin{itemize}
    \item Senza entrare troppo nel dettaglio, la fattorizzazione LU viene chiamata anche fattorizzazione di Gauss poiché è dimostrato che è possibile applicare l'algoritmo di Gauss, ovvero il \textbf{Metodo di Eliminazione di Gauss (MEG)}. 
    
    Il \textbf{calcolo dei coefficienti} dei fattori $L$ ed $U$ \textbf{richiede} circa $\dfrac{2n^{3}}{3}$ \textbf{operazioni}.
    
    \item Se la matrice $A$, cioè la matrice usata nella definizione, è \textbf{simmetrica}\footnote{Una matrice è simmetrica se coincide con la sua matrice trasposta.} e \textbf{definita positiva}\footnote{Una matrice viene \definition{definita positiva} se:
    \begin{equation*}
        \forall \mathbf{x} \in \mathbb{R}^{n} \hspace{2em} \text{con } \mathbf{x} \ne \mathbf{0}, \hspace{2em} \mathbf{x}^{T} \mathrm{A}\mathbf{x} > 0
    \end{equation*}}, è possibile trovare la \textbf{fattorizzazione di Cholesky}:
    \begin{equation}\label{eq: fattorizzazione di Cholesky}
        A = R^{T}R
    \end{equation}
    Dove $R$ è una matrice triangolare superiore con elementi positivi sulla diagonale. Inoltre, tale fattorizzazione è \textbf{unica}.

    Il \textbf{calcolo della matrice} $R$ \textbf{richiede} circa $\dfrac{n^{3}}{3}$ \textbf{operazioni} (cioè la \textbf{metà di quelle richieste per calcolare le due matrici della fattorizzazione LU}).
\end{itemize}