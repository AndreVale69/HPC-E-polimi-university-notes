\subsection{Il metodo dei minimi quadrati}

\begin{flushleft}
	\textcolor{Green3}{\faIcon{question-circle} \textbf{Perché è necessario un altro metodo?}}
\end{flushleft}
Al crescere del grado del polinomio dell'interpolazione di Lagrange, esso non garantisce una maggiore accuratezza nell'approssimazione di una funzione. Per aumentare l'accuratezza, è stato introdotto l'interpolazione Lagrangiana composita. Purtroppo, quest'ultima non è ottima per estrapolare informazioni da dati noti, ovvero \textbf{per generare nuove valutazioni in punti che giacciono al di fuori dell'intervallo di interpolazione}.

\highspace
Quindi, i precedenti metodi esposti non sono ottimi:
\begin{itemize}
	\item \textbf{Interpolazione Lagrangiana}: potrebbe soffrire del fenomeno di Runge, a causa del numero elevato di dati.
	
	\item \textbf{Interpolazione Lagrangiana composita}: terrebbe in conto solo gli ultimi $k+1$ dati, per cui non utilizzerebbe tutta la storia a disposizione e non darebbe un'espressione analiticamente semplice.
	
	\item \textbf{Interpolazione su nodi Chebyshev}: nella realtà è difficile da applicare poiché i dati a disposizione sono equispaziati.
\end{itemize}

\highspace
\begin{flushleft}
	\textcolor{Red2}{\faIcon{bookmark} \textbf{Ragionamento per arrivare alla definizione del metodo}}
\end{flushleft}
Si supponga di avere a disposizione un insieme di dati e ciascun elemento è formato da una coppia:
\begin{equation*}
	\left\{\left(x_{i}, y_{i}\right), \: i = 0, \dots, n\right\}
\end{equation*}
In cui gli $y_{i}$ potrebbero essere i valori che una funzione assume nei nodi $x_{i}$: $q\left(x_{i}\right) = y_{i}$.

\highspace
L'obbiettivo è cercare un \emph{polinomio globale} di grado $m$ (con $m \ge 1$) molto più basso rispetto a $n$ ($m \ll n$). Tale polinomio ha l'obbiettivo di \textbf{minimizzare lo \emph{scarto quadratico medio}}, ovverosia la somma dei quadrati degli errori nei nodi\footnote{Si ricorda che l'errore di un nodo è la distanza tra la funzione lineare e il suo corrispettivo interpolatore. A pagina \pageref{fig: stima dell'errore massimo dell'interpolatore Lagrangiano} è possibile vederlo visivamente.}.

\highspace
Dato lo \textbf{spazio dei polinomi di grado $m$}:
\begin{equation*}
	\mathbb{P}_{m} = \left\{p_{m} \: : \: \mathbb{R} \rightarrow \mathbb{R} \: : \: p_{m}\left(x\right) = b_{0} + b_{1}x + \cdots + b_{m}x^{m}\right\}
\end{equation*}
Allora, il (problema) \definition{metodo dei minimi quadrati} consiste nel cercare il polinomio $q \in \mathbb{P}_{m}$ tale che:
\begin{equation}\label{eq: metodo dei minimi quadrati}
	\displaystyle\sum_{i=0}^{n} \left[y_{i} - q\left(x_{i}\right)\right]^{2} \le \displaystyle\sum_{i=0}^{n} \left[y_{i} - p_{m}\left(x_{i}\right)\right]^{2} \hspace{2em} \forall p_{m} \in \mathbb{P}_{m}
\end{equation}

\highspace
Riscrivendo il polinomio $q\left(x\right)$ come:
\begin{equation*}
	q\left(x\right) = a_{0} + a_{1}x + \cdots + a_{m}x^{m}
\end{equation*}
Dove i coefficienti $a$ sono incogniti, è possibile riformulare il metodo dei minimi quadrati (eq. \ref{eq: metodo dei minimi quadrati}). Quindi determinare $a_{0}, a_{1}, \dots, a_{m}$ tali che:
\begin{equation*}
	\psi\left(a_{0}, a_{1}, \dots, a_{m}\right) = \underset{\left\{b_{i}, i = 0, \dots, m\right\}}{\min} \psi\left(b_{0}, b_{1}, \dots, b_{m}\right)
\end{equation*}
Dove:
\begin{equation*}
	 \psi\left(b_{0}, b_{1}, \dots, b_{m}\right) = \displaystyle\sum_{i=0}^{n} \left[y_{i} - \left(b_{0} + b_{1}x_{i} + \dots + b_{m}x_{i}^{m}\right)\right]^{2}
\end{equation*}
Nel caso in cui si abbia una \definition{retta di regressione} (cioè $m = 1$), il problema si riduce a:
\begin{equation*}
	\psi\left(b_{0}, b_{1}\right) = \displaystyle\sum_{i=0}^{n} \left[y_{i}^{2} + b_{0}^{2} + b_{1}^{2}x_{i}^{2} + 2b_{0}b_{1}x_{i} - 2b_{0}y_{i} - 2b_{1}x_{i}y_{i}\right]
\end{equation*}
Se ne ricava che il grafico della funzione $\psi$ è un \textbf{paraboloide convesso} il cui punto di \textbf{minimo} $\left(a_{0}, a_{1}\right)$ si trova imponendo le sue derivate parziali uguale a zero:
\begin{equation*}
	\dfrac{\partial \psi}{\partial b_{0}}\left(a_{0}, a_{1}\right) = 0 \hspace{2em} \dfrac{\partial \psi}{\partial b_{1}}\left(a_{0}, a_{1}\right) = 0
\end{equation*}
Che equivale a risolvere il seguente sistema di 2 equazioni e incognite:
\begin{equation*}
	\displaystyle \sum_{i=0}^{n} \left[a_{0} + a_{1}x_{i} - y_{i}\right] = 0 \hspace{2em} \displaystyle \sum_{i=0}^{n} \left[a_{0}x_{i} + a_{1}x_{i}^{2} - x_{i}y_{i}\right] = 0
\end{equation*}
Ponendo $D=\left(n+1\right)\displaystyle\sum_{i=0}^{n} x_{i}^{2} - \left(\displaystyle\sum_{i=0}^{n} x_{i}\right)^{2}$, la soluzione è:
\begin{equation*}
	\begin{array}{rcl}
		a_{0} &=& \dfrac{1}{D} \left[\displaystyle\sum_{i=0}^{n} y_{i} \: \displaystyle\sum_{j=0}^{n}x_{j}^{2} - \displaystyle\sum_{j=0}^{n} x_{j} \: \displaystyle\sum_{i=0}^{n}x_{i}y_{i} \right] \\ [2em]
		a_{1} &=& \dfrac{1}{D} \left[\left(n+1\right)\displaystyle\sum_{i=0}^{n}x_{i}y_{i} - \displaystyle\sum_{j=0}^{n}x_{j} \: \sum_{i=0}^{n}y_{i} \right]
	\end{array}
\end{equation*}
Il corrispondente polinomio:
\begin{equation}
	q\left(x\right) = a_{0} + a_{1}x
\end{equation}
È noto come \definition{retta dei minimi quadrati}, o \definition{retta di regressione}.

\highspace
\begin{flushleft}
	\textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{Attenzione}}
\end{flushleft}
Da notare, anche se parzialmente scontato, che nel caso in cui $m$ sia uguale a $n$ ($m=n$), \textbf{il problema si riduce all'interpolatore Lagrangiano}.